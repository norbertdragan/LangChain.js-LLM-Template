[["0",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\" class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-components/agents/index\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">\n<title data-rh=\"true\">Agents | ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/docs/components/agents/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_version\" content=\"current\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" name=\"docsearch:version\" content=\"current\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" property=\"og:title\" content=\"Agents | ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"description\" content=\"Python Guide\"><meta data-rh=\"true\" property=\"og:description\" content=\"Python Guide\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/docs/components/agents/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/agents/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/agents/\" hreflang=\"x-default\"><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">\n<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>","metadata":{"id":0}}],["1",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">","metadata":{"id":1}}],["2",{"pageContent":"<div role=\"region\" aria-label=\"Skip to main content\"><a class=\"skipToContent_fXgn\" href=\"#docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-label=\"Toggle navigation bar\" aria-expanded=\"false\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg width=\"30\" height=\"30\" viewBox=\"0 0 30 30\" aria-hidden=\"true\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" d=\"M4 7h22M4 15h22M4 23h22\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\">ğŸ¦œï¸ğŸ”— LangChain</b></a><a aria-current=\"page\" class=\"navbar__item navbar__link navbar__link--active\" href=\"/docs/\">Concepts</a><a href=\"https://python.langchain.com/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">Python Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a><a href=\"https://js.langchain.com/docs/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">JS/TS Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></div><div class=\"navbar__items navbar__items--right\"><div class=\"toggle_vylO colorModeToggle_DEke\"><button class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" type=\"button\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\"><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"lightToggleIcon_pyhR\"><path fill=\"currentColor\" d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\"></path></svg><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"darkToggleIcon_wfgR\"><path fill=\"currentColor\" d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\"></path></svg></button></div><div class=\"searchBox_ZlJk\"></div></div></div><div role=\"presentation\" class=\"navbar-sidebar__backdrop\"></div></nav><div id=\"docusaurus_skipToContent_fallback\" class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/\">Introduction</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-expanded=\"true\" href=\"/docs/category/components\">Components</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Components&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/schema/\">Schema</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Schema&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/models/\">Models</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Models&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/prompts/\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Prompts&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/indexing/\">Indexes</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Indexes&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/memory/\">Memory</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Memory&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/chains/\">Chains</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Chains&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible menu__list-item-collapsible--active\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-current=\"page\" aria-expanded=\"true\" tabindex=\"0\" href=\"/docs/components/agents/\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Agents&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/agents/tool\">Tool</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/agents/toolkit\">Toolkit</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/agents/agent\">Agent</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/agents/agent-executor\">Agent Executor</a></li></ul></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/use-cases\">Use Cases</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Use Cases&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/personal-assistants\">Personal Assistants</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-docs\">Question Answering Over Docs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/chatbots\">Chatbots</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-tabular\">Querying Tabular Data</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/apis\">Interacting with APIs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/extraction\">Extraction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/evaluation\">Evaluation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/summarization\">Summarization</a></li></ul></li></ul></nav></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/docs/category/components\"><span itemprop=\"name\">Components</span></a><meta itemprop=\"position\" content=\"1\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Agents</span><meta itemprop=\"position\" content=\"2\"></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button type=\"button\" class=\"clean-btn tocCollapsibleButton_TO0P\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>Agents</h1><div class=\"theme-admonition theme-admonition-info alert alert--info admonition_LlT9\"><div class=\"admonitionHeading_tbUL\"><span class=\"admonitionIcon_kALy\"><svg viewBox=\"0 0 14 16\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg></span>info</div><div class=\"admonitionContent_S0QG\"><p><a href=\"https://python.langchain.com/en/latest/modules/agents.html\" target=\"_blank\" rel=\"noopener noreferrer\">Python Guide</a></p><p><a href=\"https://js.langchain.com/docs/modules/agents/\" target=\"_blank\" rel=\"noopener noreferrer\">JS Guide</a></p></div></div><p>Some applications will require not just a predetermined chain of calls to LLMs/other tools,","metadata":{"id":2}}],["3",{"pageContent":"but potentially an unknown chain that depends on the user&#x27;s input.\nIn these types of chains, there is a â€œagentâ€ which has access to a suite of tools.","metadata":{"id":3}}],["4",{"pageContent":"Depending on the user input, the agent can then decide which, if any, of these tools to call.</p><p>We split the documentation into the following sections:</p><p><strong>Tools</strong></p><p>How language models interact with other resources.</p><p><strong>Agents</strong></p><p>The language model that drives decision making.</p><p><strong>Toolkits</strong></p><p>Sets of tools that when used together can accomplish a specific task.</p><p><strong>Agent Executor</strong></p><p>The logic for running agents with tools.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"go-deeper\">Go deeper<a href=\"#go-deeper\" class=\"hash-link\" aria-label=\"Direct link to Go deeper\" title=\"Direct link to Go deeper\">â€‹</a></h2><section class=\"row\"><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/agents/tool\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Tool\">ğŸ“„ï¸<!-- --> <!-- -->Tool</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/agents/toolkit\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Toolkit\">ğŸ“„ï¸<!-- --> <!-- -->Toolkit</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/agents/agent\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Agent\">ğŸ“„ï¸<!-- --> <!-- -->Agent</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/agents/agent-executor\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Agent Executor\">ğŸ“„ï¸<!-- --> <!-- -->Agent Executor</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article></section></div></article><nav class=\"pagination-nav docusaurus-mt-lg\" aria-label=\"Docs pages navigation\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/components/chains/prompt-selector\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Prompt Selector</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/components/agents/tool\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Tool</div></a></nav></div></div><div class=\"col col--3\"><div class=\"tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop\"><ul class=\"table-of-contents table-of-contents__left-border\"><li><a href=\"#go-deeper\" class=\"table-of-contents__link toc-highlight\">Go deeper</a></li></ul></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://discord.gg/cU2adEyC7w\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Discord<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://twitter.com/LangChainAI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Twitter<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchain\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Python<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchainjs\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">JS/TS<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://langchain.com\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Homepage<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://blog.langchain.dev\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Blog<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright Â© 2023 LangChain, Inc.</div></div></div></footer></div>","metadata":{"id":4}}],["5",{"pageContent":"<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":5}}],["6",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\" class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-components/chains/index\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">\n<title data-rh=\"true\">Chains | ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/docs/components/chains/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_version\" content=\"current\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" name=\"docsearch:version\" content=\"current\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" property=\"og:title\" content=\"Chains | ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"description\" content=\"Python Guide\"><meta data-rh=\"true\" property=\"og:description\" content=\"Python Guide\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/docs/components/chains/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/chains/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/chains/\" hreflang=\"x-default\"><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">\n<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>","metadata":{"id":6}}],["7",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">","metadata":{"id":7}}],["8",{"pageContent":"<div role=\"region\" aria-label=\"Skip to main content\"><a class=\"skipToContent_fXgn\" href=\"#docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-label=\"Toggle navigation bar\" aria-expanded=\"false\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg width=\"30\" height=\"30\" viewBox=\"0 0 30 30\" aria-hidden=\"true\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" d=\"M4 7h22M4 15h22M4 23h22\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\">ğŸ¦œï¸ğŸ”— LangChain</b></a><a aria-current=\"page\" class=\"navbar__item navbar__link navbar__link--active\" href=\"/docs/\">Concepts</a><a href=\"https://python.langchain.com/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">Python Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a><a href=\"https://js.langchain.com/docs/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">JS/TS Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></div><div class=\"navbar__items navbar__items--right\"><div class=\"toggle_vylO colorModeToggle_DEke\"><button class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" type=\"button\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\"><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"lightToggleIcon_pyhR\"><path fill=\"currentColor\" d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\"></path></svg><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"darkToggleIcon_wfgR\"><path fill=\"currentColor\" d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\"></path></svg></button></div><div class=\"searchBox_ZlJk\"></div></div></div><div role=\"presentation\" class=\"navbar-sidebar__backdrop\"></div></nav><div id=\"docusaurus_skipToContent_fallback\" class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/\">Introduction</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-expanded=\"true\" href=\"/docs/category/components\">Components</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Components&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/schema/\">Schema</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Schema&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/models/\">Models</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Models&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/prompts/\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Prompts&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/indexing/\">Indexes</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Indexes&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/memory/\">Memory</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Memory&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible menu__list-item-collapsible--active\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-current=\"page\" aria-expanded=\"true\" tabindex=\"0\" href=\"/docs/components/chains/\">Chains</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Chains&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/chains/chain\">Chain</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/chains/llm-chain\">LLMChain</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/chains/index_related_chains\">Index-related chains</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/chains/prompt-selector\">Prompt Selector</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/agents/\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Agents&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/use-cases\">Use Cases</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Use Cases&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/personal-assistants\">Personal Assistants</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-docs\">Question Answering Over Docs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/chatbots\">Chatbots</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-tabular\">Querying Tabular Data</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/apis\">Interacting with APIs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/extraction\">Extraction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/evaluation\">Evaluation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/summarization\">Summarization</a></li></ul></li></ul></nav></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/docs/category/components\"><span itemprop=\"name\">Components</span></a><meta itemprop=\"position\" content=\"1\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Chains</span><meta itemprop=\"position\" content=\"2\"></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button type=\"button\" class=\"clean-btn tocCollapsibleButton_TO0P\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>Chains</h1><div class=\"theme-admonition theme-admonition-info alert alert--info admonition_LlT9\"><div class=\"admonitionHeading_tbUL\"><span class=\"admonitionIcon_kALy\"><svg viewBox=\"0 0 14 16\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg></span>info</div><div class=\"admonitionContent_S0QG\"><p><a href=\"https://python.langchain.com/en/latest/modules/chains.html\" target=\"_blank\" rel=\"noopener noreferrer\">Python Guide</a></p><p><a href=\"https://js.langchain.com/docs/modules/chains/\" target=\"_blank\" rel=\"noopener noreferrer\">JS Guide</a></p></div></div><p>Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.</p><p>The most commonly used type of chain is an LLMChain, which combines a PromptTemplate, a Model, and Guardrails to take user input, format it accordingly, pass it to the model and get a response, and then validate and fix (if necessary) the model output.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"go-deeper\">Go deeper<a href=\"#go-deeper\" class=\"hash-link\" aria-label=\"Direct link to Go deeper\" title=\"Direct link to Go deeper\">â€‹</a></h2><section class=\"row\"><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/chains/chain\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Chain\">ğŸ“„ï¸<!-- --> <!-- -->Chain</h2><p class=\"text--truncate cardDescription_PWke\" title=\"A chain is just an end-to-end wrapper around multiple individual components.\">A chain is just an end-to-end wrapper around multiple individual components.</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/chains/llm-chain\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"LLMChain\">ğŸ“„ï¸<!-- --> <!-- -->LLMChain</h2><p class=\"text--truncate cardDescription_PWke\" title=\"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\">A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/chains/index_related_chains\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Index-related chains\">ğŸ“„ï¸<!-- --> <!-- -->Index-related chains</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/chains/prompt-selector\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Prompt Selector\">ğŸ“„ï¸<!-- --> <!-- -->Prompt Selector</h2><p class=\"text--truncate cardDescription_PWke\" title=\"One of the goals of chains in LangChain is to enable people to get started with a particular use case as quickly as possible. A big part of this is having good prompts.\">One of the goals of chains in LangChain is to enable people to get started with a particular use case as quickly as possible. A big part of this is having good prompts.</p></a></article></section></div></article><nav class=\"pagination-nav docusaurus-mt-lg\" aria-label=\"Docs pages navigation\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/components/memory/chat_message_history\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Chat Message History</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/components/chains/chain\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Chain</div></a></nav></div></div><div class=\"col col--3\"><div class=\"tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop\"><ul class=\"table-of-contents table-of-contents__left-border\"><li><a href=\"#go-deeper\" class=\"table-of-contents__link toc-highlight\">Go deeper</a></li></ul></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://discord.gg/cU2adEyC7w\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Discord<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://twitter.com/LangChainAI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Twitter<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchain\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Python<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchainjs\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">JS/TS<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://langchain.com\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Homepage<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://blog.langchain.dev\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Blog<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright Â© 2023 LangChain, Inc.</div></div></div></footer></div>","metadata":{"id":8}}],["9",{"pageContent":"<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":9}}],["10",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\" class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-components/indexing/index\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">\n<title data-rh=\"true\">Indexes | ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/docs/components/indexing/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_version\" content=\"current\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" name=\"docsearch:version\" content=\"current\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" property=\"og:title\" content=\"Indexes | ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"description\" content=\"Python Guide\"><meta data-rh=\"true\" property=\"og:description\" content=\"Python Guide\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/docs/components/indexing/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/indexing/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/indexing/\" hreflang=\"x-default\"><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">\n<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>","metadata":{"id":10}}],["11",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">","metadata":{"id":11}}],["12",{"pageContent":"<div role=\"region\" aria-label=\"Skip to main content\"><a class=\"skipToContent_fXgn\" href=\"#docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-label=\"Toggle navigation bar\" aria-expanded=\"false\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg width=\"30\" height=\"30\" viewBox=\"0 0 30 30\" aria-hidden=\"true\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" d=\"M4 7h22M4 15h22M4 23h22\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\">ğŸ¦œï¸ğŸ”— LangChain</b></a><a aria-current=\"page\" class=\"navbar__item navbar__link navbar__link--active\" href=\"/docs/\">Concepts</a><a href=\"https://python.langchain.com/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">Python Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a><a href=\"https://js.langchain.com/docs/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">JS/TS Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></div><div class=\"navbar__items navbar__items--right\"><div class=\"toggle_vylO colorModeToggle_DEke\"><button class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" type=\"button\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\"><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"lightToggleIcon_pyhR\"><path fill=\"currentColor\" d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\"></path></svg><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"darkToggleIcon_wfgR\"><path fill=\"currentColor\" d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\"></path></svg></button></div><div class=\"searchBox_ZlJk\"></div></div></div><div role=\"presentation\" class=\"navbar-sidebar__backdrop\"></div></nav><div id=\"docusaurus_skipToContent_fallback\" class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/\">Introduction</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-expanded=\"true\" href=\"/docs/category/components\">Components</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Components&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/schema/\">Schema</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Schema&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/models/\">Models</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Models&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/prompts/\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Prompts&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible menu__list-item-collapsible--active\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-current=\"page\" aria-expanded=\"true\" tabindex=\"0\" href=\"/docs/components/indexing/\">Indexes</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Indexes&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/indexing/document-loaders\">Document Loaders</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/indexing/text-splitters\">Text Splitters</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/indexing/retriever\">Retriever</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/indexing/vectorstore\">Vectorstore</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/memory/\">Memory</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Memory&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/chains/\">Chains</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Chains&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/agents/\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Agents&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/use-cases\">Use Cases</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Use Cases&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/personal-assistants\">Personal Assistants</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-docs\">Question Answering Over Docs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/chatbots\">Chatbots</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-tabular\">Querying Tabular Data</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/apis\">Interacting with APIs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/extraction\">Extraction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/evaluation\">Evaluation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/summarization\">Summarization</a></li></ul></li></ul></nav></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/docs/category/components\"><span itemprop=\"name\">Components</span></a><meta itemprop=\"position\" content=\"1\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Indexes</span><meta itemprop=\"position\" content=\"2\"></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button type=\"button\" class=\"clean-btn tocCollapsibleButton_TO0P\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>Indexes</h1><div class=\"theme-admonition theme-admonition-info alert alert--info admonition_LlT9\"><div class=\"admonitionHeading_tbUL\"><span class=\"admonitionIcon_kALy\"><svg viewBox=\"0 0 14 16\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg></span>info</div><div class=\"admonitionContent_S0QG\"><p><a href=\"https://python.langchain.com/en/latest/modules/indexes.html\" target=\"_blank\" rel=\"noopener noreferrer\">Python Guide</a></p><p><a href=\"https://js.langchain.com/docs/modules/indexes/\" target=\"_blank\" rel=\"noopener noreferrer\">JS Guide</a></p></div></div><p>Indexes refer to ways to structure documents so that LLMs can best interact with them.","metadata":{"id":12}}],["13",{"pageContent":"This module contains utility functions for working with documents, different types of indexes, and then examples for using those indexes in chains.</p><p>The most common way that indexes are used in chains is in a &quot;retrieval&quot; step.\nThis step refers to taking a user&#x27;s query and returning the most relevant documents.\nWe draw this distinction because (1) an index can be used for other things besides retrieval, and (2) retrieval can use other logic besides an index to find relevant documents.\nWe therefor have a concept of a &quot;Retriever&quot; interface - this is the interface that most chains work with.</p><p>Most of the time when we talk about indexes and retrieval we are talking about indexing and retrieving unstructured data (like text documents).\nFor interacting with structured data (SQL tables, etc) or APIs, please see the corresponding use case sections for links to relevant functionality.\nThe primary index and retrieval types supported by LangChain are currently centered around vector databases, and therefore","metadata":{"id":13}}],["14",{"pageContent":"a lot of the functionality we dive deep on those topics.</p><p><strong>Document Loaders</strong></p><p>Classes responsible for loading documents from various sources.</p><p><strong>Text Splitters</strong></p><p>Classes responsible for splitting text into smaller chunks.</p><p><strong>VectorStores</strong></p><p>The most common type of index. One that relies on embeddings.</p><p><strong>Retrievers</strong></p><p>Interface for fetching relevant documents to combine with language models.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"go-deeper\">Go deeper<a href=\"#go-deeper\" class=\"hash-link\" aria-label=\"Direct link to Go deeper\" title=\"Direct link to Go deeper\">â€‹</a></h2><section class=\"row\"><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/indexing/document-loaders\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Document Loaders\">ğŸ“„ï¸<!-- --> <!-- -->Document Loaders</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/indexing/text-splitters\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Text Splitters\">ğŸ“„ï¸<!-- --> <!-- -->Text Splitters</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/indexing/retriever\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Retriever\">ğŸ“„ï¸<!-- --> <!-- -->Retriever</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/indexing/vectorstore\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Vectorstore\">ğŸ“„ï¸<!-- --> <!-- -->Vectorstore</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article></section></div></article><nav class=\"pagination-nav docusaurus-mt-lg\" aria-label=\"Docs pages navigation\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/components/prompts/output-parser\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Output Parser</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/components/indexing/document-loaders\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Document Loaders</div></a></nav></div></div><div class=\"col col--3\"><div class=\"tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop\"><ul class=\"table-of-contents table-of-contents__left-border\"><li><a href=\"#go-deeper\" class=\"table-of-contents__link toc-highlight\">Go deeper</a></li></ul></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://discord.gg/cU2adEyC7w\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Discord<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://twitter.com/LangChainAI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Twitter<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchain\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Python<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchainjs\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">JS/TS<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://langchain.com\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Homepage<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://blog.langchain.dev\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Blog<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright Â© 2023 LangChain, Inc.</div></div></div></footer></div>","metadata":{"id":14}}],["15",{"pageContent":"<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":15}}],["16",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\" class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-components/memory/index\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">\n<title data-rh=\"true\">Memory | ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/docs/components/memory/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_version\" content=\"current\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" name=\"docsearch:version\" content=\"current\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" property=\"og:title\" content=\"Memory | ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"description\" content=\"Python Guide\"><meta data-rh=\"true\" property=\"og:description\" content=\"Python Guide\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/docs/components/memory/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/memory/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/memory/\" hreflang=\"x-default\"><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">\n<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>","metadata":{"id":16}}],["17",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">","metadata":{"id":17}}],["18",{"pageContent":"<div role=\"region\" aria-label=\"Skip to main content\"><a class=\"skipToContent_fXgn\" href=\"#docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-label=\"Toggle navigation bar\" aria-expanded=\"false\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg width=\"30\" height=\"30\" viewBox=\"0 0 30 30\" aria-hidden=\"true\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" d=\"M4 7h22M4 15h22M4 23h22\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\">ğŸ¦œï¸ğŸ”— LangChain</b></a><a aria-current=\"page\" class=\"navbar__item navbar__link navbar__link--active\" href=\"/docs/\">Concepts</a><a href=\"https://python.langchain.com/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">Python Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a><a href=\"https://js.langchain.com/docs/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">JS/TS Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></div><div class=\"navbar__items navbar__items--right\"><div class=\"toggle_vylO colorModeToggle_DEke\"><button class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" type=\"button\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\"><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"lightToggleIcon_pyhR\"><path fill=\"currentColor\" d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\"></path></svg><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"darkToggleIcon_wfgR\"><path fill=\"currentColor\" d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\"></path></svg></button></div><div class=\"searchBox_ZlJk\"></div></div></div><div role=\"presentation\" class=\"navbar-sidebar__backdrop\"></div></nav><div id=\"docusaurus_skipToContent_fallback\" class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/\">Introduction</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-expanded=\"true\" href=\"/docs/category/components\">Components</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Components&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/schema/\">Schema</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Schema&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/models/\">Models</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Models&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/prompts/\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Prompts&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/indexing/\">Indexes</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Indexes&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible menu__list-item-collapsible--active\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-current=\"page\" aria-expanded=\"true\" tabindex=\"0\" href=\"/docs/components/memory/\">Memory</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Memory&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/memory/chat_message_history\">Chat Message History</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/chains/\">Chains</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Chains&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/agents/\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Agents&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/use-cases\">Use Cases</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Use Cases&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/personal-assistants\">Personal Assistants</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-docs\">Question Answering Over Docs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/chatbots\">Chatbots</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-tabular\">Querying Tabular Data</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/apis\">Interacting with APIs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/extraction\">Extraction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/evaluation\">Evaluation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/summarization\">Summarization</a></li></ul></li></ul></nav></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/docs/category/components\"><span itemprop=\"name\">Components</span></a><meta itemprop=\"position\" content=\"1\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Memory</span><meta itemprop=\"position\" content=\"2\"></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button type=\"button\" class=\"clean-btn tocCollapsibleButton_TO0P\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>Memory</h1><div class=\"theme-admonition theme-admonition-info alert alert--info admonition_LlT9\"><div class=\"admonitionHeading_tbUL\"><span class=\"admonitionIcon_kALy\"><svg viewBox=\"0 0 14 16\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg></span>info</div><div class=\"admonitionContent_S0QG\"><p><a href=\"https://python.langchain.com/en/latest/modules/memory.html\" target=\"_blank\" rel=\"noopener noreferrer\">Python Guide</a></p><p><a href=\"https://js.langchain.com/docs/modules/memory/\" target=\"_blank\" rel=\"noopener noreferrer\">JS Guide</a></p></div></div><p>Memory is the concept of storing and retrieving data in the process of a conversation. There are two main methods:</p><ol><li>Based on input, fetch any relevant pieces of data</li><li>Based on the input and output, update state accordingly</li></ol><p>There are two main types of memory: short term and long term.</p><p>Short term memory generally refers to how to pass data in the context of a singular conversation (generally is previous ChatMessages or summaries of them).</p><p>Long term memory deals with how to fetch and update information between conversations.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"go-deeper\">Go deeper<a href=\"#go-deeper\" class=\"hash-link\" aria-label=\"Direct link to Go deeper\" title=\"Direct link to Go deeper\">â€‹</a></h2><section class=\"row\"><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/memory/chat_message_history\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Chat Message History\">ğŸ“„ï¸<!-- --> <!-- -->Chat Message History</h2><p class=\"text--truncate cardDescription_PWke\" title=\"The primary interface with language models at the moment in through a chat interface. The ChatMessageHistory class is responsible for remembering all previous chat interactions. These can then be passed directly back into the model, summarized in some way, or some combination.\">The primary interface with language models at the moment in through a chat interface. The ChatMessageHistory class is responsible for remembering all previous chat interactions. These can then be passed directly back into the model, summarized in some way, or some combination.</p></a></article></section></div></article><nav class=\"pagination-nav docusaurus-mt-lg\" aria-label=\"Docs pages navigation\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/components/indexing/vectorstore\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Vectorstore</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/components/memory/chat_message_history\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Chat Message History</div></a></nav></div></div><div class=\"col col--3\"><div class=\"tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop\"><ul class=\"table-of-contents table-of-contents__left-border\"><li><a href=\"#go-deeper\" class=\"table-of-contents__link toc-highlight\">Go deeper</a></li></ul></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://discord.gg/cU2adEyC7w\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Discord<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://twitter.com/LangChainAI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Twitter<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchain\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Python<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchainjs\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">JS/TS<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://langchain.com\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Homepage<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://blog.langchain.dev\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Blog<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright Â© 2023 LangChain, Inc.</div></div></div></footer></div>","metadata":{"id":18}}],["19",{"pageContent":"<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":19}}],["20",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\" class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-components/models/index\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">\n<title data-rh=\"true\">Models | ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/docs/components/models/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_version\" content=\"current\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" name=\"docsearch:version\" content=\"current\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" property=\"og:title\" content=\"Models | ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"description\" content=\"Python Guide\"><meta data-rh=\"true\" property=\"og:description\" content=\"Python Guide\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/docs/components/models/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/models/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/models/\" hreflang=\"x-default\"><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">\n<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>","metadata":{"id":20}}],["21",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">","metadata":{"id":21}}],["22",{"pageContent":"<div role=\"region\" aria-label=\"Skip to main content\"><a class=\"skipToContent_fXgn\" href=\"#docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-label=\"Toggle navigation bar\" aria-expanded=\"false\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg width=\"30\" height=\"30\" viewBox=\"0 0 30 30\" aria-hidden=\"true\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" d=\"M4 7h22M4 15h22M4 23h22\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\">ğŸ¦œï¸ğŸ”— LangChain</b></a><a aria-current=\"page\" class=\"navbar__item navbar__link navbar__link--active\" href=\"/docs/\">Concepts</a><a href=\"https://python.langchain.com/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">Python Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a><a href=\"https://js.langchain.com/docs/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">JS/TS Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></div><div class=\"navbar__items navbar__items--right\"><div class=\"toggle_vylO colorModeToggle_DEke\"><button class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" type=\"button\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\"><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"lightToggleIcon_pyhR\"><path fill=\"currentColor\" d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\"></path></svg><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"darkToggleIcon_wfgR\"><path fill=\"currentColor\" d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\"></path></svg></button></div><div class=\"searchBox_ZlJk\"></div></div></div><div role=\"presentation\" class=\"navbar-sidebar__backdrop\"></div></nav><div id=\"docusaurus_skipToContent_fallback\" class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/\">Introduction</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-expanded=\"true\" href=\"/docs/category/components\">Components</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Components&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/schema/\">Schema</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Schema&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible menu__list-item-collapsible--active\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-current=\"page\" aria-expanded=\"true\" tabindex=\"0\" href=\"/docs/components/models/\">Models</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Models&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/models/language-model\">Language Model</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/models/chat-model\">Chat Model</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/models/text-embedding-model\">Text Embedding Model</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/prompts/\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Prompts&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/indexing/\">Indexes</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Indexes&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/memory/\">Memory</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Memory&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/chains/\">Chains</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Chains&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/agents/\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Agents&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/use-cases\">Use Cases</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Use Cases&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/personal-assistants\">Personal Assistants</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-docs\">Question Answering Over Docs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/chatbots\">Chatbots</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-tabular\">Querying Tabular Data</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/apis\">Interacting with APIs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/extraction\">Extraction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/evaluation\">Evaluation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/summarization\">Summarization</a></li></ul></li></ul></nav></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/docs/category/components\"><span itemprop=\"name\">Components</span></a><meta itemprop=\"position\" content=\"1\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Models</span><meta itemprop=\"position\" content=\"2\"></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button type=\"button\" class=\"clean-btn tocCollapsibleButton_TO0P\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>Models</h1><div class=\"theme-admonition theme-admonition-info alert alert--info admonition_LlT9\"><div class=\"admonitionHeading_tbUL\"><span class=\"admonitionIcon_kALy\"><svg viewBox=\"0 0 14 16\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg></span>info</div><div class=\"admonitionContent_S0QG\"><p><a href=\"https://python.langchain.com/en/latest/modules/models.html\" target=\"_blank\" rel=\"noopener noreferrer\">Python Guide</a></p><p><a href=\"https://js.langchain.com/docs/modules/models/\" target=\"_blank\" rel=\"noopener noreferrer\">JS Guide</a></p></div></div><p>This section of the documentation deals with different types of models that are used in LangChain.","metadata":{"id":22}}],["23",{"pageContent":"On this page we will go over the model types at a high level,\nbut we have individual pages for each model type.</p><p><strong>LLMs</strong></p><p>Large Language Models (LLMs) are the first type of models we cover.\nThese models take a text string as input, and return a text string as output.</p><p><strong>Chat Models</strong></p><p>Chat Models are the second type of models we cover.\nThese models are usually backed by a language model, but their APIs are more structured.\nSpecifically, these models take a list of Chat Messages as input, and return a Chat Message.</p><p><strong>Text Embedding Models</strong></p><p>The third type of models we cover are text embedding models.","metadata":{"id":23}}],["24",{"pageContent":"These models take text as input and return a list of floats.</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"go-deeper\">Go deeper<a href=\"#go-deeper\" class=\"hash-link\" aria-label=\"Direct link to Go deeper\" title=\"Direct link to Go deeper\">â€‹</a></h2><section class=\"row\"><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/models/language-model\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Language Model\">ğŸ“„ï¸<!-- --> <!-- -->Language Model</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/models/chat-model\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Chat Model\">ğŸ“„ï¸<!-- --> <!-- -->Chat Model</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/models/text-embedding-model\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Text Embedding Model\">ğŸ“„ï¸<!-- --> <!-- -->Text Embedding Model</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article></section></div></article><nav class=\"pagination-nav docusaurus-mt-lg\" aria-label=\"Docs pages navigation\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/components/schema/document\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Document</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/components/models/language-model\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Language Model</div></a></nav></div></div><div class=\"col col--3\"><div class=\"tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop\"><ul class=\"table-of-contents table-of-contents__left-border\"><li><a href=\"#go-deeper\" class=\"table-of-contents__link toc-highlight\">Go deeper</a></li></ul></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://discord.gg/cU2adEyC7w\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Discord<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://twitter.com/LangChainAI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Twitter<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchain\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Python<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchainjs\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">JS/TS<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://langchain.com\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Homepage<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://blog.langchain.dev\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Blog<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright Â© 2023 LangChain, Inc.</div></div></div></footer></div>","metadata":{"id":24}}],["25",{"pageContent":"<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":25}}],["26",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\" class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-components/prompts/index\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">\n<title data-rh=\"true\">Prompts | ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/docs/components/prompts/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_version\" content=\"current\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" name=\"docsearch:version\" content=\"current\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" property=\"og:title\" content=\"Prompts | ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"description\" content=\"Python Guide\"><meta data-rh=\"true\" property=\"og:description\" content=\"Python Guide\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/docs/components/prompts/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/prompts/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/prompts/\" hreflang=\"x-default\"><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">\n<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>","metadata":{"id":26}}],["27",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">","metadata":{"id":27}}],["28",{"pageContent":"<div role=\"region\" aria-label=\"Skip to main content\"><a class=\"skipToContent_fXgn\" href=\"#docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-label=\"Toggle navigation bar\" aria-expanded=\"false\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg width=\"30\" height=\"30\" viewBox=\"0 0 30 30\" aria-hidden=\"true\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" d=\"M4 7h22M4 15h22M4 23h22\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\">ğŸ¦œï¸ğŸ”— LangChain</b></a><a aria-current=\"page\" class=\"navbar__item navbar__link navbar__link--active\" href=\"/docs/\">Concepts</a><a href=\"https://python.langchain.com/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">Python Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a><a href=\"https://js.langchain.com/docs/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">JS/TS Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></div><div class=\"navbar__items navbar__items--right\"><div class=\"toggle_vylO colorModeToggle_DEke\"><button class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" type=\"button\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\"><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"lightToggleIcon_pyhR\"><path fill=\"currentColor\" d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\"></path></svg><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"darkToggleIcon_wfgR\"><path fill=\"currentColor\" d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\"></path></svg></button></div><div class=\"searchBox_ZlJk\"></div></div></div><div role=\"presentation\" class=\"navbar-sidebar__backdrop\"></div></nav><div id=\"docusaurus_skipToContent_fallback\" class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/\">Introduction</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-expanded=\"true\" href=\"/docs/category/components\">Components</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Components&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/schema/\">Schema</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Schema&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/models/\">Models</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Models&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible menu__list-item-collapsible--active\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-current=\"page\" aria-expanded=\"true\" tabindex=\"0\" href=\"/docs/components/prompts/\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Prompts&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/prompts/prompt-value\">Prompt Value</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/prompts/prompt-template\">Prompt Template</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/prompts/example-selectors\">Example Selectors</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/prompts/output-parser\">Output Parser</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/indexing/\">Indexes</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Indexes&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/memory/\">Memory</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Memory&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/chains/\">Chains</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Chains&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/agents/\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Agents&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/use-cases\">Use Cases</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Use Cases&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/personal-assistants\">Personal Assistants</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-docs\">Question Answering Over Docs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/chatbots\">Chatbots</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-tabular\">Querying Tabular Data</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/apis\">Interacting with APIs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/extraction\">Extraction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/evaluation\">Evaluation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/summarization\">Summarization</a></li></ul></li></ul></nav></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/docs/category/components\"><span itemprop=\"name\">Components</span></a><meta itemprop=\"position\" content=\"1\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Prompts</span><meta itemprop=\"position\" content=\"2\"></li></ul></nav><div class=\"tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo\"><button type=\"button\" class=\"clean-btn tocCollapsibleButton_TO0P\">On this page</button></div><div class=\"theme-doc-markdown markdown\"><h1>Prompts</h1><div class=\"theme-admonition theme-admonition-info alert alert--info admonition_LlT9\"><div class=\"admonitionHeading_tbUL\"><span class=\"admonitionIcon_kALy\"><svg viewBox=\"0 0 14 16\"><path fill-rule=\"evenodd\" d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg></span>info</div><div class=\"admonitionContent_S0QG\"><p><a href=\"https://python.langchain.com/en/latest/modules/prompts.html\" target=\"_blank\" rel=\"noopener noreferrer\">Python Guide</a></p><p><a href=\"https://js.langchain.com/docs/modules/prompts/\" target=\"_blank\" rel=\"noopener noreferrer\">JS Guide</a></p></div></div><p>The new way of programming models is through prompts.","metadata":{"id":28}}],["29",{"pageContent":"A &quot;prompt&quot; refers to the input to the model.\nThis input is rarely hard coded, but rather is often constructed from multiple components.\nA PromptTemplate is responsible for the construction of this input.\nLangChain provides several classes and functions to make constructing and working with prompts easy.</p><p>This section of documentation is split into four sections:</p><p><strong>PromptValue</strong></p><p>The class representing an input to a model.</p><p><strong>Prompt Templates</strong></p><p>The class in charge of constructing a PromptValue.</p><p><strong>Example Selectors</strong></p><p>Often times it is useful to include examples in prompts.\nThese examples can be hardcoded, but it is often more powerful if they are dynamically selected.</p><p><strong>Output Parsers</strong></p><p>Language models (and Chat Models) output text.\nBut many times you may want to get more structured information than just text back.\nThis is where output parsers come in.\nOutput Parsers are responsible for (1) instructing the model how output should be formatted,","metadata":{"id":29}}],["30",{"pageContent":"(2) parsing output into the desired formatting (including retrying if necessary).</p><h2 class=\"anchor anchorWithStickyNavbar_LWe7\" id=\"go-deeper\">Go deeper<a href=\"#go-deeper\" class=\"hash-link\" aria-label=\"Direct link to Go deeper\" title=\"Direct link to Go deeper\">â€‹</a></h2><section class=\"row\"><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/prompts/prompt-value\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Prompt Value\">ğŸ“„ï¸<!-- --> <!-- -->Prompt Value</h2><p class=\"text--truncate cardDescription_PWke\" title=\"A â€œpromptâ€ refers to what is passed to the underlying model. The main abstractions have for prompt in LangChain so for all deal with text data. For other data types (images, audio) we are working on adding abstractions but do not yet have them.\">A â€œpromptâ€ refers to what is passed to the underlying model. The main abstractions have for prompt in LangChain so for all deal with text data. For other data types (images, audio) we are working on adding abstractions but do not yet have them.</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/prompts/prompt-template\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Prompt Template\">ğŸ“„ï¸<!-- --> <!-- -->Prompt Template</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/prompts/example-selectors\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Example Selectors\">ğŸ“„ï¸<!-- --> <!-- -->Example Selectors</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/prompts/output-parser\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Output Parser\">ğŸ“„ï¸<!-- --> <!-- -->Output Parser</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Python Guide\">Python Guide</p></a></article></section></div></article><nav class=\"pagination-nav docusaurus-mt-lg\" aria-label=\"Docs pages navigation\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/components/models/text-embedding-model\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Text Embedding Model</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/components/prompts/prompt-value\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Prompt Value</div></a></nav></div></div><div class=\"col col--3\"><div class=\"tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop\"><ul class=\"table-of-contents table-of-contents__left-border\"><li><a href=\"#go-deeper\" class=\"table-of-contents__link toc-highlight\">Go deeper</a></li></ul></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://discord.gg/cU2adEyC7w\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Discord<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://twitter.com/LangChainAI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Twitter<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchain\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Python<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchainjs\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">JS/TS<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://langchain.com\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Homepage<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://blog.langchain.dev\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Blog<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright Â© 2023 LangChain, Inc.</div></div></div></footer></div>","metadata":{"id":30}}],["31",{"pageContent":"<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":31}}],["32",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\" class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-components/schema/index\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">\n<title data-rh=\"true\">Schema | ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/docs/components/schema/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_version\" content=\"current\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" name=\"docsearch:version\" content=\"current\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" property=\"og:title\" content=\"Schema | ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"description\" content=\"This section covers the basic data types and schemas that are used throughout the codebase.\"><meta data-rh=\"true\" property=\"og:description\" content=\"This section covers the basic data types and schemas that are used throughout the codebase.\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/docs/components/schema/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/schema/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/components/schema/\" hreflang=\"x-default\"><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">","metadata":{"id":32}}],["33",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">","metadata":{"id":33}}],["34",{"pageContent":"<div role=\"region\" aria-label=\"Skip to main content\"><a class=\"skipToContent_fXgn\" href=\"#docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-label=\"Toggle navigation bar\" aria-expanded=\"false\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg width=\"30\" height=\"30\" viewBox=\"0 0 30 30\" aria-hidden=\"true\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" d=\"M4 7h22M4 15h22M4 23h22\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\">ğŸ¦œï¸ğŸ”— LangChain</b></a><a aria-current=\"page\" class=\"navbar__item navbar__link navbar__link--active\" href=\"/docs/\">Concepts</a><a href=\"https://python.langchain.com/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">Python Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a><a href=\"https://js.langchain.com/docs/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">JS/TS Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></div><div class=\"navbar__items navbar__items--right\"><div class=\"toggle_vylO colorModeToggle_DEke\"><button class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" type=\"button\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\"><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"lightToggleIcon_pyhR\"><path fill=\"currentColor\" d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\"></path></svg><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"darkToggleIcon_wfgR\"><path fill=\"currentColor\" d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\"></path></svg></button></div><div class=\"searchBox_ZlJk\"></div></div></div><div role=\"presentation\" class=\"navbar-sidebar__backdrop\"></div></nav><div id=\"docusaurus_skipToContent_fallback\" class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link\" href=\"/docs/\">Introduction</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-expanded=\"true\" href=\"/docs/category/components\">Components</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Components&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item\"><div class=\"menu__list-item-collapsible menu__list-item-collapsible--active\"><a class=\"menu__link menu__link--sublist menu__link--active\" aria-current=\"page\" aria-expanded=\"true\" tabindex=\"0\" href=\"/docs/components/schema/\">Schema</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Schema&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/schema/text\">Text</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/schema/chat-messages\">ChatMessages</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/schema/examples\">Examples</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/components/schema/document\">Document</a></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/models/\">Models</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Models&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/prompts/\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Prompts&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/indexing/\">Indexes</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Indexes&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/memory/\">Memory</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Memory&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/chains/\">Chains</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Chains&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/agents/\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Agents&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/use-cases\">Use Cases</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Use Cases&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/personal-assistants\">Personal Assistants</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-docs\">Question Answering Over Docs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/chatbots\">Chatbots</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-tabular\">Querying Tabular Data</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/apis\">Interacting with APIs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/extraction\">Extraction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/evaluation\">Evaluation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/summarization\">Summarization</a></li></ul></li></ul></nav></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item\"><a class=\"breadcrumbs__link\" itemprop=\"item\" href=\"/docs/category/components\"><span itemprop=\"name\">Components</span></a><meta itemprop=\"position\" content=\"1\"></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Schema</span><meta itemprop=\"position\" content=\"2\"></li></ul></nav><div class=\"theme-doc-markdown markdown\"><h1>Schema</h1><p>This section covers the basic data types and schemas that are used throughout the codebase.</p><section class=\"row\"><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/schema/text\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Text\">ğŸ“„ï¸<!-- --> <!-- -->Text</h2><p class=\"text--truncate cardDescription_PWke\" title=\"When working with language models, the primary interface through which you can interact with them is through text. As an over simplification, a lot of models are &quot;text in, text out&quot;. Therefor, a lot of the interfaces in LangChain are centered around text.\">When working with language models, the primary interface through which you can interact with them is through text. As an over simplification, a lot of models are &quot;text in, text out&quot;. Therefor, a lot of the interfaces in LangChain are centered around text.</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/schema/chat-messages\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"ChatMessages\">ğŸ“„ï¸<!-- --> <!-- -->ChatMessages</h2><p class=\"text--truncate cardDescription_PWke\" title=\"The primary interface through which end users interact with these is a chat interface. For this reason, some model providers even started providing access to the underlying API in a way that expects chat messages. These messages have a content field (which is usually text) and are associated with a user. Right now the supported users are System, Human, and AI.\">The primary interface through which end users interact with these is a chat interface. For this reason, some model providers even started providing access to the underlying API in a way that expects chat messages. These messages have a content field (which is usually text) and are associated with a user. Right now the supported users are System, Human, and AI.</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/schema/examples\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Examples\">ğŸ“„ï¸<!-- --> <!-- -->Examples</h2><p class=\"text--truncate cardDescription_PWke\" title=\"Examples are input/output pairs that represent inputs to a function and then expected output. They can be used in both training and evaluation of models.\">Examples are input/output pairs that represent inputs to a function and then expected output. They can be used in both training and evaluation of models.</p></a></article><article class=\"col col--6 margin-bottom--lg\"><a class=\"card padding--lg cardContainer_fWXF\" href=\"/docs/components/schema/document\"><h2 class=\"text--truncate cardTitle_rnsV\" title=\"Document\">ğŸ“„ï¸<!-- --> <!-- -->Document</h2><p class=\"text--truncate cardDescription_PWke\" title=\"A piece of unstructured data. Consists of page_content (the content of the data) and metadata (auxiliary pieces of information describing attributes of the data).\">A piece of unstructured data. Consists of page_content (the content of the data) and metadata (auxiliary pieces of information describing attributes of the data).</p></a></article></section></div></article><nav class=\"pagination-nav docusaurus-mt-lg\" aria-label=\"Docs pages navigation\"><a class=\"pagination-nav__link pagination-nav__link--prev\" href=\"/docs/category/components\"><div class=\"pagination-nav__sublabel\">Previous</div><div class=\"pagination-nav__label\">Components</div></a><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/components/schema/text\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Text</div></a></nav></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://discord.gg/cU2adEyC7w\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Discord<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://twitter.com/LangChainAI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Twitter<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchain\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Python<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchainjs\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">JS/TS<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://langchain.com\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Homepage<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://blog.langchain.dev\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Blog<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright Â© 2023 LangChain, Inc.</div></div></div></footer></div>","metadata":{"id":34}}],["35",{"pageContent":"<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":35}}],["36",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\" class=\"docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-index\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">\n<title data-rh=\"true\">ğŸ¦œï¸ğŸ”— LangChain | ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/docs/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_version\" content=\"current\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" name=\"docsearch:version\" content=\"current\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"docs-default-current\"><meta data-rh=\"true\" property=\"og:title\" content=\"ğŸ¦œï¸ğŸ”— LangChain | ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"description\" content=\"LangChain is a framework for developing applications powered by language models.\"><meta data-rh=\"true\" property=\"og:description\" content=\"LangChain is a framework for developing applications powered by language models.\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/docs/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/docs/\" hreflang=\"x-default\"><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">\n<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">","metadata":{"id":36}}],["37",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">","metadata":{"id":37}}],["38",{"pageContent":"<div role=\"region\" aria-label=\"Skip to main content\"><a class=\"skipToContent_fXgn\" href=\"#docusaurus_skipToContent_fallback\">Skip to main content</a></div><nav aria-label=\"Main\" class=\"navbar navbar--fixed-top\"><div class=\"navbar__inner\"><div class=\"navbar__items\"><button aria-label=\"Toggle navigation bar\" aria-expanded=\"false\" class=\"navbar__toggle clean-btn\" type=\"button\"><svg width=\"30\" height=\"30\" viewBox=\"0 0 30 30\" aria-hidden=\"true\"><path stroke=\"currentColor\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" d=\"M4 7h22M4 15h22M4 23h22\"></path></svg></button><a class=\"navbar__brand\" href=\"/\"><b class=\"navbar__title text--truncate\">ğŸ¦œï¸ğŸ”— LangChain</b></a><a aria-current=\"page\" class=\"navbar__item navbar__link navbar__link--active\" href=\"/docs/\">Concepts</a><a href=\"https://python.langchain.com/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">Python Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a><a href=\"https://js.langchain.com/docs/\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"navbar__item navbar__link\">JS/TS Docs<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></div><div class=\"navbar__items navbar__items--right\"><div class=\"toggle_vylO colorModeToggle_DEke\"><button class=\"clean-btn toggleButton_gllP toggleButtonDisabled_aARS\" type=\"button\" disabled=\"\" title=\"Switch between dark and light mode (currently light mode)\" aria-label=\"Switch between dark and light mode (currently light mode)\" aria-live=\"polite\"><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"lightToggleIcon_pyhR\"><path fill=\"currentColor\" d=\"M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z\"></path></svg><svg viewBox=\"0 0 24 24\" width=\"24\" height=\"24\" class=\"darkToggleIcon_wfgR\"><path fill=\"currentColor\" d=\"M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z\"></path></svg></button></div><div class=\"searchBox_ZlJk\"></div></div></div><div role=\"presentation\" class=\"navbar-sidebar__backdrop\"></div></nav><div id=\"docusaurus_skipToContent_fallback\" class=\"main-wrapper mainWrapper_z2l0 docsWrapper_BCFX\"><button aria-label=\"Scroll back to top\" class=\"clean-btn theme-back-to-top-button backToTopButton_sjWU\" type=\"button\"></button><div class=\"docPage__5DB\"><aside class=\"theme-doc-sidebar-container docSidebarContainer_b6E3\"><div class=\"sidebarViewport_Xe31\"><div class=\"sidebar_njMd\"><nav aria-label=\"Docs sidebar\" class=\"menu thin-scrollbar menu_SIkG\"><ul class=\"theme-doc-sidebar-menu menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item\"><a class=\"menu__link menu__link--active\" aria-current=\"page\" href=\"/docs/\">Introduction</a></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/components\">Components</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Components&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/schema/\">Schema</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Schema&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/models/\">Models</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Models&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/prompts/\">Prompts</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Prompts&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/indexing/\">Indexes</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Indexes&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/memory/\">Memory</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Memory&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/chains/\">Chains</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Chains&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"false\" tabindex=\"0\" href=\"/docs/components/agents/\">Agents</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Agents&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div></li></ul></li><li class=\"theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item\"><div class=\"menu__list-item-collapsible\"><a class=\"menu__link menu__link--sublist\" aria-expanded=\"true\" href=\"/docs/category/use-cases\">Use Cases</a><button aria-label=\"Toggle the collapsible sidebar category &#x27;Use Cases&#x27;\" type=\"button\" class=\"clean-btn menu__caret\"></button></div><ul style=\"display:block;overflow:visible;height:auto\" class=\"menu__list\"><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/personal-assistants\">Personal Assistants</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-docs\">Question Answering Over Docs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/chatbots\">Chatbots</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/qa-tabular\">Querying Tabular Data</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/apis\">Interacting with APIs</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/extraction\">Extraction</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/evaluation\">Evaluation</a></li><li class=\"theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item\"><a class=\"menu__link\" tabindex=\"0\" href=\"/docs/use-cases/summarization\">Summarization</a></li></ul></li></ul></nav></div></div></aside><main class=\"docMainContainer_gTbr\"><div class=\"container padding-top--md padding-bottom--lg\"><div class=\"row\"><div class=\"col docItemCol_VOVn\"><div class=\"docItemContainer_Djhp\"><article><nav class=\"theme-doc-breadcrumbs breadcrumbsContainer_Z_bl\" aria-label=\"Breadcrumbs\"><ul class=\"breadcrumbs\" itemscope=\"\" itemtype=\"https://schema.org/BreadcrumbList\"><li class=\"breadcrumbs__item\"><a aria-label=\"Home page\" class=\"breadcrumbs__link\" href=\"/\"><svg viewBox=\"0 0 24 24\" class=\"breadcrumbHomeIcon_YNFT\"><path d=\"M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z\" fill=\"currentColor\"></path></svg></a></li><li itemscope=\"\" itemprop=\"itemListElement\" itemtype=\"https://schema.org/ListItem\" class=\"breadcrumbs__item breadcrumbs__item--active\"><span class=\"breadcrumbs__link\" itemprop=\"name\">Introduction</span><meta itemprop=\"position\" content=\"1\"></li></ul></nav><div class=\"theme-doc-markdown markdown\"><h1>ğŸ¦œï¸ğŸ”— LangChain</h1><p>LangChain is a framework for developing applications powered by language models.","metadata":{"id":38}}],["39",{"pageContent":"We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also:</p><ol><li>Be data-aware: connect a language model to other sources of data</li><li>Be agentic: Allow a language model to interact with its environment</li></ol><p>As such, the LangChain framework is designed with the objective in mind to enable those types of applications.</p><p>There are two main value props the LangChain framework provides:</p><ol><li>Components: LangChain provides modular abstractions for the components neccessary to work with language models. LangChain also has collections of implementations for all these abstractions. The components are designed to be easy to use, regardless of whether you are using the rest of the LangChain framework or not.</li><li>Use-Case Specific Chains: Chains can be thought of as assembling these components in particular ways in order to best accomplish a particular use case. These are intended to be a higher level interface through which people can easily get started with a specific use case. These chains are also designed to be customizable.</li></ol><p>Accordingly, we split the following documentation into those two value props. In this documentation, we go over components and use cases at high level and in a language-agnostic way. For language-specific ways of using these components and tackling these use cases, please see the language-specific sections linked at the top of the page.</p></div></article><nav class=\"pagination-nav docusaurus-mt-lg\" aria-label=\"Docs pages navigation\"><a class=\"pagination-nav__link pagination-nav__link--next\" href=\"/docs/category/components\"><div class=\"pagination-nav__sublabel\">Next</div><div class=\"pagination-nav__label\">Components</div></a></nav></div></div></div></div></main></div></div><footer class=\"footer\"><div class=\"container container-fluid\"><div class=\"row footer__links\"><div class=\"col footer__col\"><div class=\"footer__title\">Community</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://discord.gg/cU2adEyC7w\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Discord<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://twitter.com/LangChainAI\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Twitter<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">GitHub</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchain\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Python<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://github.com/hwchase17/langchainjs\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">JS/TS<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div><div class=\"col footer__col\"><div class=\"footer__title\">More</div><ul class=\"footer__items clean-list\"><li class=\"footer__item\"><a href=\"https://langchain.com\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Homepage<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li><li class=\"footer__item\"><a href=\"https://blog.langchain.dev\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"footer__link-item\">Blog<svg width=\"13.5\" height=\"13.5\" aria-hidden=\"true\" viewBox=\"0 0 24 24\" class=\"iconExternalLink_nPIU\"><path fill=\"currentColor\" d=\"M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z\"></path></svg></a></li></ul></div></div><div class=\"footer__bottom text--center\"><div class=\"footer__copyright\">Copyright Â© 2023 LangChain, Inc.</div></div></div></footer></div>","metadata":{"id":39}}],["40",{"pageContent":"<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":40}}],["41",{"pageContent":"<!doctype html>\n<html lang=\"en\" dir=\"ltr\">\n<head>\n<meta charset=\"UTF-8\">\n<meta name=\"generator\" content=\"Docusaurus v2.3.1\">","metadata":{"id":41}}],["42",{"pageContent":"<title data-rh=\"true\">ğŸ¦œï¸ğŸ”— LangChain</title><meta data-rh=\"true\" property=\"og:title\" content=\"ğŸ¦œï¸ğŸ”— LangChain\"><meta data-rh=\"true\" name=\"viewport\" content=\"width=device-width,initial-scale=1\"><meta data-rh=\"true\" name=\"twitter:card\" content=\"summary_large_image\"><meta data-rh=\"true\" property=\"og:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" name=\"twitter:image\" content=\"https://docs.langchain.com/img/parrot-chainlink-icon.jpg\"><meta data-rh=\"true\" property=\"og:url\" content=\"https://docs.langchain.com/\"><meta data-rh=\"true\" name=\"docusaurus_locale\" content=\"en\"><meta data-rh=\"true\" name=\"docusaurus_tag\" content=\"default\"><meta data-rh=\"true\" name=\"docsearch:language\" content=\"en\"><meta data-rh=\"true\" name=\"docsearch:docusaurus_tag\" content=\"default\"><link data-rh=\"true\" rel=\"icon\" href=\"/img/favicon.ico\"><link data-rh=\"true\" rel=\"canonical\" href=\"https://docs.langchain.com/\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/\" hreflang=\"en\"><link data-rh=\"true\" rel=\"alternate\" href=\"https://docs.langchain.com/\" hreflang=\"x-default\"><script data-rh=\"true\">function maybeInsertBanner(){window.__DOCUSAURUS_INSERT_BASEURL_BANNER&&insertBanner()}function insertBanner(){var n=document.getElementById(\"docusaurus-base-url-issue-banner-container\");if(n){n.innerHTML='\\n<div id=\"docusaurus-base-url-issue-banner\" style=\"border: thick solid red; background-color: rgb(255, 230, 179); margin: 20px; padding: 20px; font-size: 20px;\">\\n   <p style=\"font-weight: bold; font-size: 30px;\">Your Docusaurus site did not load properly.</p>\\n   <p>A very common reason is a wrong site <a href=\"https://docusaurus.io/docs/docusaurus.config.js/#baseurl\" style=\"font-weight: bold;\">baseUrl configuration</a>.</p>\\n   <p>Current configured baseUrl = <span style=\"font-weight: bold; color: red;\">/</span>  (default value)</p>\\n   <p>We suggest trying baseUrl = <span id=\"docusaurus-base-url-issue-banner-suggestion-container\" style=\"font-weight: bold; color: green;\"></span></p>\\n</div>\\n';var e=document.getElementById(\"docusaurus-base-url-issue-banner-suggestion-container\"),s=window.location.pathname,r=\"/\"===s.substr(-1)?s:s+\"/\";e.innerHTML=r}}window.__DOCUSAURUS_INSERT_BASEURL_BANNER=!0,document.addEventListener(\"DOMContentLoaded\",maybeInsertBanner)</script><link rel=\"stylesheet\" href=\"/assets/css/styles.04b80185.css\">","metadata":{"id":42}}],["43",{"pageContent":"<link rel=\"preload\" href=\"/assets/js/runtime~main.5d811302.js\" as=\"script\">\n<link rel=\"preload\" href=\"/assets/js/main.38b88b56.js\" as=\"script\">\n</head>\n<body class=\"navigation-with-keyboard\">\n<script>!function(){function t(t){document.documentElement.setAttribute(\"data-theme\",t)}var e=function(){var t=null;try{t=localStorage.getItem(\"theme\")}catch(t){}return t}();t(null!==e?e:\"light\")}()</script><div id=\"__docusaurus\">\n<div id=\"docusaurus-base-url-issue-banner-container\"></div></div>\n<script src=\"/assets/js/runtime~main.5d811302.js\"></script>\n<script src=\"/assets/js/main.38b88b56.js\"></script>\n</body>\n</html>","metadata":{"id":43}}],["44",{"pageContent":"# Helicone\n\nThis page covers how to use the [Helicone](https://helicone.ai) within LangChain.\n\n## What is Helicone?\n\nHelicone is an [open source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\n\n![Helicone](/img/HeliconeDashboard.png)\n\n## Quick start\n\nWith your LangChain environment you can just add the following parameter.\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\nNow head over to [helicone.ai](https://helicone.ai/onboarding?step=2) to create your account, and add your OpenAI API key within our dashboard to view your logs.\n\n![Helicone](/img/HeliconeKeys.png)\n\n## How to enable Helicone caching\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n    baseOptions: {\n      headers: {\n        \"Helicone-Cache-Enabled\": \"true\",\n      },\n    },\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\n[Helicone caching docs](https://docs.helicone.ai/advanced-usage/caching)\n\n## How to use Helicone custom properties\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n    baseOptions: {\n      headers: {\n        \"Helicone-Property-Session\": \"24\",\n        \"Helicone-Property-Conversation\": \"support_issue_2\",\n        \"Helicone-Property-App\": \"mobile\",\n      },\n    },\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\n[Helicone property docs](https://docs.helicone.ai/advanced-usage/custom-properties)","metadata":{"id":44}}],["45",{"pageContent":"---\nsidebar_position: 3\n---\n\n# Quickstart, using Chat Models\n\nChat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they expose is a bit different.\nRather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n\nChat model APIs are fairly new, so we are still figuring out the correct abstractions.\n\n## Installation and Setup\n\nTo get started, follow the [installation instructions](./install) to install LangChain.\n\n## Getting Started\n\nThis section covers how to get started with chat models. The interface is based around messages rather than raw text.\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models\";\nimport { HumanChatMessage, SystemChatMessage } from \"langchain/schema\";\n\nconst chat = new ChatOpenAI({ temperature: 0 });\n```\n\nHere we create a chat model using the API key stored in the environment variable `OPENAI_API_KEY`. We'll be calling this chat model throughout this section.\n\n### Chat Models: Message in, Message out\n\nYou can get chat completions by passing one or more messages to the chat model. The response will also be a message. The types of messages currently supported in LangChain are `AIChatMessage`, `HumanChatMessage`, `SystemChatMessage`, and a generic `ChatMessage` -- ChatMessage takes in an arbitrary role parameter, which we won't be using here. Most of the time, you'll just be dealing with `HumanChatMessage`, `AIChatMessage`, and `SystemChatMessage`.\n\n```typescript\nconst response = await chat.call([\n  new HumanChatMessage(\n    \"Translate this sentence from English to French. I love programming.\"\n  ),\n]);\n\nconsole.log(response);\n```\n\n```\nAIChatMessage { text: \"J'aime programmer.\" }\n```\n\n#### Multiple Messages","metadata":{"id":45}}],["46",{"pageContent":"new HumanChatMessage(\n    \"Translate this sentence from English to French. I love programming.\"\n  ),\n]);\n\nconsole.log(response);\n```\n\n```\nAIChatMessage { text: \"J'aime programmer.\" }\n```\n\n#### Multiple Messages\n\nOpenAI's chat-based models (currently `gpt-3.5-turbo` and `gpt-4`) support multiple messages as input. See [here](https://platform.openai.com/docs/guides/chat/chat-vs-completions) for more information. Here is an example of sending a system and user message to the chat model:\n\n```typescript\nresponse = await chat.call([\n  new SystemChatMessage(\n    \"You are a helpful assistant that translates English to French.\"\n  ),\n  new HumanChatMessage(\"Translate: I love programming.\"),\n]);\n\nconsole.log(response);\n```\n\n```\nAIChatMessage { text: \"J'aime programmer.\" }\n```\n\n#### Multiple Completions\n\nYou can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter.\n\n```typescript\nconst responseC = await chat.generate([\n  [\n    new SystemChatMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanChatMessage(\n      \"Translate this sentence from English to French. I love programming.\"\n    ),\n  ],\n  [\n    new SystemChatMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n    new HumanChatMessage(\n      \"Translate this sentence from English to French. I love artificial intelligence.\"\n    ),\n  ],\n]);\n\nconsole.log(responseC);\n```\n\n```\n{\n  generations: [\n    [\n      {\n        text: \"J'aime programmer.\",\n        message: AIChatMessage { text: \"J'aime programmer.\" },\n      }\n    ],\n    [\n      {\n        text: \"J'aime l'intelligence artificielle.\",\n        message: AIChatMessage { text: \"J'aime l'intelligence artificielle.\" }\n      }\n    ]\n  ]\n}\n```\n\n### Chat Prompt Templates: Manage Prompts for Chat Models","metadata":{"id":46}}],["47",{"pageContent":"message: AIChatMessage { text: \"J'aime l'intelligence artificielle.\" }\n      }\n    ]\n  ]\n}\n```\n\n### Chat Prompt Templates: Manage Prompts for Chat Models\n\nYou can make use of templating by using a `MessagePromptTemplate`. You can build a `ChatPromptTemplate` from one or more `MessagePromptTemplates`. You can use `ChatPromptTemplate`'s `formatPromptValue` -- this returns a `PromptValue`, which you can convert to a string or Message object, depending on whether you want to use the formatted value as input to an llm or chat model.\n\nContinuing with the previous example:\n\n```typescript\nimport {\n  SystemMessagePromptTemplate,\n  HumanMessagePromptTemplate,\n  ChatPromptTemplate,\n} from \"langchain/prompts\";\n```\n\nFirst we create a reusable template:\n\n```typescript\nconst translationPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n  ),\n  HumanMessagePromptTemplate.fromTemplate(\"{text}\"),\n]);\n```\n\nThen we can use the template to generate a response:\n\n```typescript\nconst responseA = await chat.generatePrompt([\n  await translationPrompt.formatPromptValue({\n    input_language: \"English\",\n    output_language: \"French\",\n    text: \"I love programming.\",\n  }),\n]);\n\nconsole.log(responseA);\n```\n\n```\n{\n  generations: [\n    [\n      {\n        text: \"J'aime programmer.\",\n        message: AIChatMessage { text: \"J'aime programmer.\" }\n      }\n    ]\n  ]\n}\n```\n\n### Model + Prompt = LLMChain\n\nThis pattern of asking for the completion of a formatted prompt is quite common, so we introduce the next piece of the puzzle: LLMChain\n\n```typescript\nconst chain = new LLMChain({\n  prompt: chatPrompt,\n  llm: chat,\n});\n```\n\nThen you can call the chain:\n\n```typescript\nconst responseB = await chain.call({\n  input_language: \"English\",\n  output_language: \"French\",\n  text: \"I love programming.\",\n});\n\nconsole.log(responseB);\n```\n\n```\n{ text: \"J'aime programmer.\" }\n```","metadata":{"id":47}}],["48",{"pageContent":"```typescript\nconst responseB = await chain.call({\n  input_language: \"English\",\n  output_language: \"French\",\n  text: \"I love programming.\",\n});\n\nconsole.log(responseB);\n```\n\n```\n{ text: \"J'aime programmer.\" }\n```\n\nThe chain will internally accumulate the messages sent to the model, and the ones received as output. Then it will inject the messages into the prompt on the next call. So you can call the chain a few times, and it remembers previous messages:\n\n```typescript\nconst responseD = await chain.call({\n  input: \"hi from London, how are you doing today\",\n});\n```\n\n```\n{\n  response: \"Hello! As an AI language model, I don't have feelings, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\"\n}\n```\n\n```typescript\nconst responseE = await chain.call({\n  input: \"Do you know where I am?\",\n});\n\nconsole.log(responseE);\n```\n\n```\n{\n  response: \"Yes, you mentioned that you are from London. However, as an AI language model, I don't have access to your current location unless you provide me with that information.\"\n}\n```\n\n### Agents: Dynamically Run Chains Based on User Input\n\nFinally, we introduce Tools and Agents, which extend the model with other abilities, such as search, or a calculator.\n\nA tool is a function that takes a string (such as a search query) and returns a string (such as a search result). They also have a name and description, which are used by the chat model to identify which tool it should call.\n\n```typescript\nclass Tool {\n  name: string;\n  description: string;\n  call(arg: string): Promise<string>;\n}\n```\n\nAn agent is a stateless wrapper around an agent prompt chain (such as MRKL) which takes care of formatting tools into the prompt, as well as parsing the responses obtained from the chat model.\n\n```typescript\ninterface AgentStep {\n  action: AgentAction;\n  observation: string;\n}\n\ninterface AgentAction {\n  tool: string; // Tool.name\n  toolInput: string; // Tool.call argument\n}\n\ninterface AgentFinish {\n  returnValues: object;\n}\n\nclass Agent {","metadata":{"id":48}}],["49",{"pageContent":"action: AgentAction;\n  observation: string;\n}\n\ninterface AgentAction {\n  tool: string; // Tool.name\n  toolInput: string; // Tool.call argument\n}\n\ninterface AgentFinish {\n  returnValues: object;\n}\n\nclass Agent {\n  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;\n}\n```\n\nTo make agents more powerful we need to make them iterative, ie. call the model multiple times until they arrive at the final answer. That's the job of the AgentExecutor.\n\n```typescript\nclass AgentExecutor {\n  // a simplified implementation\n  run(inputs: object) {\n    const steps = [];\n    while (true) {\n      const step = await this.agent.plan(steps, inputs);\n      if (step instanceof AgentFinish) {\n        return step.returnValues;\n      }\n      steps.push(step);\n    }\n  }\n}\n```\n\nAnd finally, we can use the AgentExecutor to run an agent:\n\n```typescript\n// Define the list of tools the agent can use\nconst tools = [new SerpAPI()];\n// Create the agent from the chat model and the tools\nconst agent = ChatAgent.fromLLMAndTools(new ChatOpenAI(), tools);\n// Create an executor, which calls to the agent until an answer is found\nconst executor = AgentExecutor.fromAgentAndTools({ agent, tools });\n\nconst responseG = await executor.run(\n  \"How many people live in canada as of 2023?\"\n);\n\nconsole.log(responseG);\n```\n\n```\n38,626,704.\n```\n\n### Memory: Add State to Chains and Agents\n\nYou can also use the chain to store state. This is useful for eg. chatbots, where you want to keep track of the conversation history. MessagesPlaceholder is a special prompt template that will be replaced with the messages passed in each call.\n\n```typescript\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"\n  ),\n  new MessagesPlaceholder(\"history\"),","metadata":{"id":49}}],["50",{"pageContent":"),\n  new MessagesPlaceholder(\"history\"),\n  HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n]);\n\nconst chain = new ConversationChain({\n  memory: new BufferMemory({ returnMessages: true, memoryKey: \"history\" }),\n  prompt: chatPrompt,\n  llm: chat,\n});\n```\n\n## Streaming\n\nYou can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support `tokenUsage` reporting while streaming is enabled.\n\n```typescript\nlet s = \"\";\nconst chatStreaming = new ChatOpenAI({\n  streaming: true,\n  callbackManager: CallbackManager.fromHandlers({\n    async handleLLMNewToken(token: string) {\n      console.clear();\n      s += token;\n      console.log(s);\n    },\n  }),\n});\n\nconst responseD = await chatStreaming.call([\n  new HumanChatMessage(\"Write me a song about sparkling water.\"),\n]);\n```\n\n```\nVerse 1:\nBubbles in the bottle,\nLight and refreshing,\nIt's the drink that I love,\nMy thirst quenching blessing.\n\nChorus:\nSparkling water, my fountain of youth,\nI can't get enough, it's the perfect truth,\nIt's fizzy and fun, and oh so clear,\nSparkling water, it's crystal clear.\n\nVerse 2:\nNo calories or sugars,\nJust a burst of delight,\nIt's the perfect cooler,\nOn a hot summer night.\n\nChorus:\nSparkling water, my fountain of youth,\nI can't get enough, it's the perfect truth,\nIt's fizzy and fun, and oh so clear,\nSparkling water, it's crystal clear.\n\nBridge:\nIt's my happy place,\nIn every situation,\nMy daily dose of hydration,\nAlways bringing satisfaction.\n\nChorus:\nSparkling water, my fountain of youth,\nI can't get enough, it's the perfect truth,\nIt's fizzy and fun, and oh so clear,\nSparkling water, it's crystal clear.\n\nOutro:\nSparkling water, it's crystal clear,\nMy love for you will never disappear.\n```","metadata":{"id":50}}],["51",{"pageContent":"---\nsidebar_position: 2\n---\n\n# Quickstart, using LLMs\n\nThis tutorial gives you a quick walkthrough about building an end-to-end language model application with LangChain.\n\n## Installation and Setup\n\nTo get started, follow the [installation instructions](./install) to install LangChain.\n\n## Picking up a LLM\n\nUsing LangChain will usually require integrations with one or more model providers, data stores, apis, etc.\n\nFor this example, we will be using OpenAI's APIs, so no additional setup is required.\n\n## Building a Language Model Application\n\nNow that we have installed LangChain, we can start building our language model application.\n\nLangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications.\n\n### LLMs: Get Predictions from a Language Model\n\nThe most basic building block of LangChain is calling an LLM on some input. Let's walk through a simple example of how to do this. For this purpose, let's pretend we are building a service that generates a company name based on what the company makes.\n\nIn order to do this, we first need to import the LLM wrapper.\n\n```typescript\nimport { OpenAI } from \"langchain\";\n```\n\nWe will then need to set the environment variable for the OpenAI key. Three options here:\n\n1. We can do this by setting the value in a `.env` file and use the [dotenv](https://github.com/motdotla/dotenv) package to read it.\n\n```bash\nOPENAI_API_KEY=\"...\"\n```\n\n2. Or we can export the environment variable with the following command in your shell:\n\n```bash\nexport OPENAI_API_KEY=sk-....\n```\n\n3. Or we can do it when initializing the wrapper along with other arguments. In this example, we probably want the outputs to be MORE random, so we'll initialize it with a HIGH temperature.\n\n```typescript\nconst model = new OpenAI({ openAIApiKey: \"sk-...\", temperature: 0.9 });\n```\n\nOnce we have initialized the wrapper, we can now call it on some input!\n\n```typescript\nconst res = await model.call(","metadata":{"id":51}}],["52",{"pageContent":"```typescript\nconst model = new OpenAI({ openAIApiKey: \"sk-...\", temperature: 0.9 });\n```\n\nOnce we have initialized the wrapper, we can now call it on some input!\n\n```typescript\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log(res);\n```\n\n```shell\n{ res: '\\n\\nFantasy Sockery' }\n```\n\n### Prompt Templates: Manage Prompts for LLMs\n\nCalling an LLM is a great first step, but it's just the beginning. Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you are probably taking user input and constructing a prompt, and then sending that to the LLM.\n\nFor example, in the previous example, the text we passed in was hardcoded to ask for a name for a company that made colorful socks. In this imaginary service, what we would want to do is take only the user input describing what the company does, and then format the prompt with that information.\n\nThis is easy to do with LangChain!\n\nFirst lets define the prompt template:\n\n```typescript\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst template = \"What is a good name for a company that makes {product}?\";\nconst prompt = new PromptTemplate({\n  template: template,\n  inputVariables: [\"product\"],\n});\n```\n\nLet's now see how this works! We can call the `.format` method to format it.\n\n```typescript\nconst res = await prompt.format({ product: \"colorful socks\" });\nconsole.log(res);\n```\n\n```shell\n{ res: 'What is a good name for a company that makes colorful socks?' }\n```\n\n### Chains: Combine LLMs and Prompts in Multi-Step Workflows\n\nUp until now, we've worked with the PromptTemplate and LLM primitives by themselves. But of course, a real application is not just one primitive, but rather a combination of them.\n\nA chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.\n\nThe most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM.","metadata":{"id":52}}],["53",{"pageContent":"A chain in LangChain is made up of links, which can be either primitives like LLMs or other chains.\n\nThe most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM.\n\nExtending the previous example, we can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM.\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst model = new OpenAI({ temperature: 0.9 });\nconst template = \"What is a good name for a company that makes {product}?\";\nconst prompt = new PromptTemplate({\n  template: template,\n  inputVariables: [\"product\"],\n});\n```\n\nWe can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM:\n\n```typescript\nimport { LLMChain } from \"langchain/chains\";\n\nconst chain = new LLMChain({ llm: model, prompt: prompt });\n```\n\nNow we can run that chain only specifying the product!\n\n```typescript\nconst res = await chain.call({ product: \"colorful socks\" });\nconsole.log(res);\n```\n\n```shell\n{ res: { text: '\\n\\nColorfulCo Sockery.' } }\n```\n\nThere we go! There's the first chain - an LLM Chain. This is one of the simpler types of chains, but understanding how it works will set you up well for working with more complex chains.\n\n### Agents: Dynamically Run Chains Based on User Input\n\nSo far the chains we've looked at run in a predetermined order.\n\nAgents no longer do: they use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n\nWhen used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.\n\nIn order to load agents, you should understand the following concepts:","metadata":{"id":53}}],["54",{"pageContent":"In order to load agents, you should understand the following concepts:\n\n- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, code REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n- LLM: The language model powering the agent.\n- Agent: The agent to use. This should be a string that references a support agent class. Because this tutorial focuses on the simplest, highest level API, this only covers using the standard supported agents.\n\nFor this example, you'll need to set the SerpAPI environment variables in the `.env` file.\n\n```bash\nSERPAPI_API_KEY=\"...\"\n```\n\nNow we can get started!\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport { initializeAgentExecutor } from \"langchain/agents\";\nimport { SerpAPI, Calculator } from \"langchain/tools\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [new SerpAPI(), new Calculator()];\n\nconst executor = await initializeAgentExecutor(\n  tools,\n  model,\n  \"zero-shot-react-description\"\n);\nconsole.log(\"Loaded agent.\");\n\nconst input =\n  \"Who is Olivia Wilde's boyfriend?\" +\n  \" What is his current age raised to the 0.23 power?\";\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n```\n\n```shell\nlangchain-examples:start: Executing with input \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"...\nlangchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.\n```\n\n### Memory: Add State to Chains and Agents","metadata":{"id":54}}],["55",{"pageContent":"langchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.\n```\n\n### Memory: Add State to Chains and Agents\n\nSo far, all the chains and agents we've gone through have been stateless. But often, you may want a chain or agent to have some concept of \"memory\" so that it may remember information about its previous interactions. The clearest and simple example of this is when designing a chatbot - you want it to remember previous messages so it can use context from that to have a better conversation. This would be a type of \"short-term memory\". On the more complex side, you could imagine a chain/agent remembering key pieces of information over time - this would be a form of \"long-term memory\".\n\nLangChain provides several specially created chains just for this purpose. This section walks through using one of those chains (the `ConversationChain`).\n\nBy default, the `ConversationChain` has a simple type of memory that remembers all previous inputs/outputs and adds them to the context that is passed. Let's take a look at using this chain.\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferMemory();\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log(res1);\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log(res2);\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```\n\n## Streaming","metadata":{"id":55}}],["56",{"pageContent":"const res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log(res2);\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```\n\n## Streaming\n\nYou can also use the streaming API to get words streamed back to you as they are generated. This is useful for eg. chatbots, where you want to show the user what is being generated as it is being generated. Note: OpenAI as of this writing does not support `tokenUsage` reporting while streaming is enabled.\n\n```typescript\nconst chat = new OpenAI({\n  streaming: true,\n  callbackManager: CallbackManager.fromHandlers({\n    async handleLLMNewToken(token: string) {\n      console.log(token);\n    },\n  }),\n});\n\nconst response = await chat.call(\"Write me a song about sparkling water.\");\nconsole.log(response);\n```\n\n```\nVerse 1\n\nOn a hot summer day, I'm looking for a treat\nI'm thirsty for something cool and sweet\nWhen I open up the fridge, what do I see?\nA bottle of sparkling water, it's calling out to me\n\nChorus\n\nSparkling water, it's so refreshing\nIt quenches my thirst, it's the perfect thing\nIt's so light and bubbly, it's like a dream\nAnd I'm loving every sip of sparkling water\n\nVerse 2\n\nI take it out of the fridge and pour some in a glass\nIt's so light and bubbly, I can feel the fizz\nI take a sip and suddenly I'm revived\nThis sparkling water's just what I need to survive\n\nChorus\n\nSparkling water, it's so refreshing\nIt quenches my thirst, it's the perfect thing\nIt's so light and bubbly, it's like a dream\nAnd I'm loving every sip of sparkling water\n\nBridge\n\nIt's like drinking sunshine between my hands\nIt's so light and bubbly, I'm in a trance\nThe summer heat's no match for sparkling water\nIt's my favorite\n```","metadata":{"id":56}}],["57",{"pageContent":"---\nsidebar_position: 1\n---\n\n# Setup and Installation\n\n## Quickstart\n\nTo get started with Langchain, you'll need to initialize a new Node.js project and configure some scripts to build, format, and compile your code.\n\nIf you just want to get started quickly, [clone this repository](https://github.com/domeccleston/langchain-ts-starter) and follow the README instructions for a boilerplate project with those dependencies set up.\n\nIf you'd prefer to set things up yourself, read on for instructions.\n\n## Installation\n\nTo get started, install LangChain with the following command:\n\n```bash npm2yarn\nnpm install -S langchain\n```\n\nWe currently support LangChain on Node.js 18 and 19. Go [here](https://github.com/hwchase17/langchainjs/discussions/152) to vote on the next environment we should support.\n\n### TypeScript\n\nLangChain is written in TypeScript and provides type definitions for all of its public APIs.\n\n## Loading the library\n\n### ESM in Node.js\n\nLangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\n```\n\nIf you are using TypeScript in an ESM project we suggest updating your `tsconfig.json` to include the following:\n\n```json\n{\n  \"compilerOptions\": {\n    ...\n    \"target\": \"ES2020\", // or higher\n    \"module\": \"nodenext\",\n  }\n}\n```\n\n### CommonJS in Node.js\n\nLangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:\n\n```typescript\nconst { OpenAI } = require(\"langchain/llms\");\n```\n\n### Other environments\n\nLangChain currently supports only Node.js-based environments. This includes Vercel Serverless functions (but not Edge functions), as well as other serverless environments, like AWS Lambda and Google Cloud Functions.","metadata":{"id":57}}],["58",{"pageContent":"We currently do not support running LangChain in the browser. We are listening to the community on additional environments that we should support. Go [here](https://github.com/hwchase17/langchainjs/discussions/152) to vote and discuss the next environments we should support.\n\nPlease see [Deployment](../production/deployment.md) for more information on deploying LangChain applications.\n\n## Unsupported: Node.js 16\n\nWe do not support Node.js 16, but if you want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.\n\nYou will have to make `fetch` available globally, either:\n\n- run your application with `NODE_OPTIONS='--experimental-fetch' node ...`, or\n- install `node-fetch` and follow the instructions [here](https://github.com/node-fetch/node-fetch#providing-global-access)\n\nAdditionally you'll have to polyfill `unstructuredClone`, eg. by installing `core-js` and following the instructions [here](https://github.com/zloirock/core-js).\n\nIf you are running this on Node.js 18 or 19, you do not need to do anything.","metadata":{"id":58}}],["59",{"pageContent":"# Welcome to LangChain\n\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also:\n\n- _Be data-aware_: connect a language model to other sources of data\n- _Be agentic_: allow a language model to interact with its environment\n\nThe LangChain framework is designed with the above principles in mind.\n\n## Getting Started\n\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\n\n- [Getting Started Documentation](./getting-started/guide-llm.mdx)\n\n## Components\n\nThere are several main modules that LangChain provides support for. For each module we provide some examples to get started and get familiar with some of the concepts. These modules are, in increasing order of complexity:\n\n- Prompts: This includes prompt management, prompt optimization, and prompt serialization.\n\n- LLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\n\n- Indexes: This includes patterns and functionality for structuring your own text data so it can interact with language models (including embeddings, vectorstores, text splitters, retrievers, etc).\n\n- Memory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\n\n- Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.","metadata":{"id":59}}],["60",{"pageContent":"- Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\n\n## Reference Docs\n\n---\n\nAll of LangChain's reference documentation, in one place. Full documentation on all methods and classes.\n\n## Production\n\n---\n\nAs you move from prototyping into production, we're developing resources to help you do so.\nThese including:\n\n- Deployment: resources on how to deploy your end application.\n- Tracing: resouces on how to use tracing to log and debug your applications.\n\n## Additional Resources\n\n---\n\nAdditional collection of resources we think may be useful as you develop your application!\n\n- [LangChainHub](https://github.com/hwchase17/langchain-hub): The LangChainHub is a place to share and explore other prompts, chains, and agents.\n\n- [Discord](https://discord.gg/6adMQxSpJS): Join us on our Discord to discuss all things LangChain!\n\n- [Production Support](https://forms.gle/57d8AmXBYp8PP8tZA): As you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out this form and we'll set up a dedicated support Slack channel.","metadata":{"id":60}}],["61",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/custom_llm_agent_chat.ts\";\n\n# Custom LLM Agent (with Chat Model)\n\nThis example covers how to create a custom Agent powered by a Chat Model.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":61}}],["62",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/custom_llm_agent.ts\";\n\n# Custom LLM Agent\n\nThis example covers how to create a custom Agent powered by an LLM.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":62}}],["63",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/chat_mrkl.ts\";\n\n# MRKL Agent for Chat Models\n\nThis example covers how to use an agent that uses the ReAct Framework (based on the descriptions of tools) to decide what action to take. This agent is optimized to be used with Chat Models. If you want to use it with an LLM, you can use the [LLM MRKL Agent](./llm_mrkl) instead.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":63}}],["64",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/chat_convo_with_tracing.ts\";\n\n# Conversational Agent\n\nThis example covers how to create a conversational agent for a chat model. It will utilize chat specific prompts.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n````\nLoaded agent.\nEntering new agent_executor chain...\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"Hello Bob! How can I assist you today?\"\n}\nFinished chain.\nGot output Hello Bob! How can I assist you today?\nEntering new agent_executor chain...\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"Your name is Bob.\"\n}\nFinished chain.\nGot output Your name is Bob.\nEntering new agent_executor chain...\n```json\n{\n    \"action\": \"search\",\n    \"action_input\": \"weather in pomfret\"\n}\n```\nA steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\n```json\n{\n    \"action\": \"Final Answer\",\n    \"action_input\": \"The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\"\n}\n```\nFinished chain.\nGot output The weather in Pomfret is a steady rain early...then remaining cloudy with a few showers. High 48F. Winds WNW at 10 to 15 mph. Chance of rain 80%.\n````","metadata":{"id":64}}],["65",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chat/agent.ts\";\n\n# Agent with Custom Prompt, using Chat Models\n\nThis example covers how to create a custom agent for a chat model. It will utilize chat specific prompts.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":65}}],["66",{"pageContent":"---\nsidebar_label: Examples\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Agents\n\n<DocCardList />","metadata":{"id":66}}],["67",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/mrkl.ts\";\n\n# MRKL Agent for LLMs\n\nThis example covers how to use an agent that uses the ReAct Framework (based on the descriptions of tools) to decide what action to take. This agent is optimized to be used with LLMs. If you want to use it with a chat model, try the [Chat MRKL Agent](./chat_mrkl).\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":67}}],["68",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Agents\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/agent)\n:::\n\nAn agent is a stateless wrapper around an agent prompt chain (such as MRKL) which takes care of formatting tools into the prompt, as well as parsing the responses obtained from the chat model. It takes in user input and returns a response corresponding to an â€œactionâ€ to take and a corresponding â€œaction inputâ€.\n\n```typescript\ninterface AgentStep {\n  action: AgentAction;\n  observation: string;\n}\n\ninterface AgentAction {\n  tool: string; // Tool.name\n  toolInput: string; // Tool.call argument\n}\n\ninterface AgentFinish {\n  returnValues: object;\n}\n\nclass Agent {\n  plan(steps: AgentStep[], inputs: object): Promise<AgentAction | AgentFinish>;\n}\n```\n\n## Which agent to choose?\n\nThe agent you choose depends on the type of task you want to perform. Here's a quick guide to help you pick the right agent for your use case:\n\n- If you're using a text LLM, first try `zero-shot-react-description`, aka. the [MRKL agent for LLMs](./examples/llm_mrkl).\n- If you're using a Chat Model, try `chat-zero-shot-react-description`, aka. the [MRKL agent for Chat Models](./examples/chat_mrkl).\n- If you're using a Chat Model and want to use memory, try `chat-conversational-react-description`, the [Conversational agent](./examples/conversational_agent).\n\n## All Agents\n\n<DocCardList />","metadata":{"id":68}}],["69",{"pageContent":"---\nsidebar_label: Getting Started\nhide_table_of_contents: true\n---\n\n# Getting Started: Agent Executors\n\nAgents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n\nWhen used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.\n\nIn order to load agents, you should understand the following concepts:\n\n- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, code REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n- LLM: The language model powering the agent.\n- Agent: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents.\n\nFor this example, you'll need to set the SerpAPI environment variables in the `.env` file.\n\n```bash\nSERPAPI_API_KEY=\"...\"\n```\n\nNow we can get started!\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport { initializeAgentExecutor } from \"langchain/agents\";\nimport { SerpAPI, Calculator } from \"langchain/tools\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [new SerpAPI(), new Calculator()];\n\nconst executor = await initializeAgentExecutor(\n  tools,\n  model,\n  \"zero-shot-react-description\"\n);\nconsole.log(\"Loaded agent.\");\n\nconst input =\n  \"Who is Olivia Wilde's boyfriend?\" +\n  \" What is his current age raised to the 0.23 power?\";\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n```\n\n```shell\nlangchain-examples:start: Executing with input \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"...","metadata":{"id":69}}],["70",{"pageContent":"console.log(`Got output ${result.output}`);\n```\n\n```shell\nlangchain-examples:start: Executing with input \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"...\nlangchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.\n```","metadata":{"id":70}}],["71",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Agent Executors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/agent-executor)\n:::\n\nTo make agents more powerful we need to make them iterative, ie. call the model multiple times until they arrive at the final answer. That's the job of the AgentExecutor.\n\n```typescript\nclass AgentExecutor {\n  // a simplified implementation\n  run(inputs: object) {\n    const steps = [];\n    while (true) {\n      const step = await this.agent.plan(steps, inputs);\n      if (step instanceof AgentFinish) {\n        return step.returnValues;\n      }\n      steps.push(step);\n    }\n  }\n}\n```\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"id":71}}],["72",{"pageContent":"---\nsidebar_position: 7\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Agents\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents)\n:::\n\n<DocCardList />","metadata":{"id":72}}],["73",{"pageContent":"---\nsidebar_label: Examples\nhide_table_of_contents: true\n---\n\n# Examples: Toolkits\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"id":73}}],["74",{"pageContent":"# JSON Agent Toolkit\n\nThis example shows how to load and use an agent with a JSON toolkit.\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { JsonToolkit, createJsonAgent } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const toolkit = new JsonToolkit(new JsonSpec(data));\n  const model = new OpenAI({ temperature: 0 });\n  const executor = createJsonAgent(model, toolkit);\n\n  const input = `What are the required parameters in the request body to the /completions endpoint?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"id":74}}],["75",{"pageContent":"# OpenAPI Agent Toolkit\n\nThis example shows how to load and use an agent with a OpenAPI toolkit.\n\n```typescript\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { createOpenApiAgent, OpenApiToolkit } from \"langchain/agents\";\nimport { OpenAI } from \"langchain\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const headers = {\n    \"Content-Type\": \"application/json\",\n    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,\n  };\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new OpenApiToolkit(new JsonSpec(data), model, headers);\n  const executor = createOpenApiAgent(model, toolkit);\n\n  const input = `Make a POST request to openai /completions. The prompt should be 'tell me a joke.'`;\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"id":75}}],["76",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# SQL Agent Toolkit\n\nThis example shows how to load and use an agent with a SQL toolkit.\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/sql.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":76}}],["77",{"pageContent":"# VectorStore Agent Toolkit\n\nThis example shows how to load and use an agent with a vectorstore toolkit.\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport { HNSWLib } from \"langchain/vectorstores\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport {\n  VectorStoreToolkit,\n  createVectorStoreAgent,\n  VectorStoreInfo,\n} from \"langchain/agents\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n\n  /* Create the agent */\n  const vectorStoreInfo: VectorStoreInfo = {\n    name: \"state_of_union_address\",\n    description: \"the most recent state of the Union address\",\n    vectorStore,\n  };\n\n  const toolkit = new VectorStoreToolkit(vectorStoreInfo, model);\n  const agent = createVectorStoreAgent(model, toolkit);\n\n  const input =\n    \"What did biden say about Ketanji Brown Jackson is the state of the union address?\";\n  console.log(`Executing: ${input}`);\n  const result = await agent.call({ input });\n  console.log(`Got output ${result.output}`);\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"id":77}}],["78",{"pageContent":"---\nsidebar_label: Toolkits\nsidebar_position: 2\nhide_table_of_contents: true\n---\n\n# Getting Started: Toolkits\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/toolkit)\n:::\n\nGroups of [tools](../tools/) that can be used/are necessary to solve a particular problem.\n\n```typescript\ninterface Toolkit {\n  tools: Tool[];\n}\n```\n\n## All Toolkits\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"id":78}}],["79",{"pageContent":"# Agents with Vector Stores\n\nThis notebook covers how to combine agents and vector stores. The use case for this is that youâ€™ve ingested your data into a vector store and want to interact with it in an agentic manner.\n\nThe recommended method for doing so is to create a VectorDBQAChain and then use that as a tool in the overall agent. Letâ€™s take a look at doing this below. You can do this with multiple different vector databases, and use the agent as a way to choose between them. There are two different ways of doing this - you can either let the agent use the vector stores as normal tools, or you can set `returnDirect: true` to just use the agent as a router.\n\nFirst, you'll want to import the relevant modules:\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport { initializeAgentExecutor } from \"langchain/agents\";\nimport { SerpAPI, Calculator, ChainTool } from \"langchain/tools\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\n```\n\nNext, you'll want to create the vector store with your data, and then the QA chain to interact with that vector store.\n\n```typescript\nconst model = new OpenAI({ temperature: 0 });\n/* Load in the file we want to do question answering over */\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n/* Split the text into chunks */\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n/* Create the vectorstore */\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n/* Create the chain */\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore);\n```\n\nNow that you have that chain, you can create a tool to use that chain. Note that you should update the name and description to be specific to your QA chain.\n\n```typescript\nconst qaTool = new ChainTool({","metadata":{"id":79}}],["80",{"pageContent":"Now that you have that chain, you can create a tool to use that chain. Note that you should update the name and description to be specific to your QA chain.\n\n```typescript\nconst qaTool = new ChainTool({\n  name: \"state-of-union-qa\",\n  description:\n    \"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.\",\n  chain: chain,\n});\n```\n\nNow you can construct and using the tool just as you would any other!\n\n```typescript\nconst tools = [new SerpAPI(), new Calculator(), qaTool];\n\nconst executor = await initializeAgentExecutor(\n  tools,\n  model,\n  \"zero-shot-react-description\"\n);\nconsole.log(\"Loaded agent.\");\n\nconst input = `What did biden say about ketanji brown jackson is the state of the union address?`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);\n```\n\nYou can also set `returnDirect: true` if you intend to use the agent as a router and just want to directly return the result of the VectorDBQAChain.\n\n```typescript\nconst qaTool = new ChainTool({\n  name: \"state-of-union-qa\",\n  description:\n    \"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.\",\n  chain: chain,\n  returnDirect: true,\n});\n```","metadata":{"id":80}}],["81",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/aiplugin-tool.ts\";\n\n# ChatGPT Plugins\n\nThis example shows how to use ChatGPT Plugins within LangChain abstractions.\n\nNote 1: This currently only works for plugins with no auth.\n\nNote 2: There are almost certainly other ways to do this, this is just a first pass. If you have better ideas, please open a PR!\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n````\nEntering new agent_executor chain...\nThought: Klarna is a payment provider, not a store. I need to check if there is a Klarna Shopping API that I can use to search for t-shirts.\nAction:\n```\n\n{\n\"action\": \"KlarnaProducts\",\n\"action_input\": \"\"\n}\n\n```\n\nUsage Guide: Use the Klarna plugin to get relevant product suggestions for any shopping or researching purpose. The query to be sent should not include stopwords like articles, prepositions and determinants. The api works best when searching for words that are related to products, like their name, brand, model or category. Links will always be returned and should be shown to the user.","metadata":{"id":81}}],["82",{"pageContent":"OpenAPI Spec: {\"openapi\":\"3.0.1\",\"info\":{\"version\":\"v0\",\"title\":\"Open AI Klarna product Api\"},\"servers\":[{\"url\":\"https://www.klarna.com/us/shopping\"}],\"tags\":[{\"name\":\"open-ai-product-endpoint\",\"description\":\"Open AI Product Endpoint. Query for products.\"}],\"paths\":{\"/public/openai/v0/products\":{\"get\":{\"tags\":[\"open-ai-product-endpoint\"],\"summary\":\"API for fetching Klarna product information\",\"operationId\":\"productsUsingGET\",\"parameters\":[{\"name\":\"q\",\"in\":\"query\",\"description\":\"query, must be between 2 and 100 characters\",\"required\":true,\"schema\":{\"type\":\"string\"}},{\"name\":\"size\",\"in\":\"query\",\"description\":\"number of products returned\",\"required\":false,\"schema\":{\"type\":\"integer\"}},{\"name\":\"budget\",\"in\":\"query\",\"description\":\"maximum price of the matching product in local currency, filters results\",\"required\":false,\"schema\":{\"type\":\"integer\"}}],\"responses\":{\"200\":{\"description\":\"Products found\",\"content\":{\"application/json\":{\"schema\":{\"$ref\":\"#/components/schemas/ProductResponse\"}}}},\"503\":{\"description\":\"one or more services are unavailable\"}},\"deprecated\":false}}},\"components\":{\"schemas\":{\"Product\":{\"type\":\"object\",\"properties\":{\"attributes\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"string\"},\"url\":{\"type\":\"string\"}},\"title\":\"Product\"},\"ProductResponse\":{\"type\":\"object\",\"properties\":{\"products\":{\"type\":\"array\",\"items\":{\"$ref\":\"#/components/schemas/Product\"}}},\"title\":\"ProductResponse\"}}}}\nNow that I know there is a Klarna Shopping API, I can use it to search for t-shirts. I will make a GET request to the API with the query parameter \"t-shirt\".\nAction:\n```\n\n{\n\"action\": \"requests_get\",\n\"action_input\": \"https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt\"\n}\n\n```","metadata":{"id":82}}],["83",{"pageContent":"Action:\n```\n\n{\n\"action\": \"requests_get\",\n\"action_input\": \"https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt\"\n}\n\n```\n\n\n{\"products\":[{\"name\":\"Psycho Bunny Mens Copa Gradient Logo Graphic Tee\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?source=openai\",\"price\":\"$35.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:White,Blue,Black,Orange\"]},{\"name\":\"T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203506327/Clothing/T-shirt/?source=openai\",\"price\":\"$20.45\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,White,Blue,Black,Orange\"]},{\"name\":\"Palm Angels Bear T-shirt - Black\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?source=openai\",\"price\":\"$168.36\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Black\"]},{\"name\":\"Tommy Hilfiger Essential Flag Logo T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201840629/Clothing/Tommy-Hilfiger-Essential-Flag-Logo-T-shirt/?source=openai\",\"price\":\"$22.52\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Red,Gray,White,Blue,Black\",\"Pattern:Solid Color\",\"Environmental Attributes :Organic\"]},{\"name\":\"Coach Outlet Signature T Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203005573/Clothing/Coach-Outlet-Signature-T-Shirt/?source=openai\",\"price\":\"$75.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray\"]}]}\nFinished chain.\n{\n  result: {\n    output: 'The available t-shirts in Klarna are Psycho Bunny Mens Copa Gradient Logo Graphic Tee, T-shirt, Palm Angels Bear T-shirt - Black, Tommy Hilfiger Essential Flag Logo T-shirt, and Coach Outlet Signature T Shirt.',\n    intermediateSteps: [ [Object], [Object] ]\n  }\n}\n````","metadata":{"id":83}}],["84",{"pageContent":"---\nsidebar_position: 1\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Tools\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/agents/tool)\n:::\n\nA tool is an abstraction around a function that makes it easy for a language model to interact with it. Specifically, the interface of a tool has a single text input and a single text output. It includes a name and description that communicate to the [Model](../../models/) what the tool does and when to use it.\n\n```typescript\ninterface Tool {\n  call(arg: string): Promise<string>;\n\n  name: string;\n\n  description: string;\n}\n```\n\n## All Tools\n\n<DocCardList />\n\n## Advanced\n\nTo implement a custom tool you can subclass the `Tool` class and implement the `_call` method. The `_call` method is called with the input text and should return the output text. The Tool superclass implements the `call` method, which takes care of calling the right CallbackManager methods before and after calling your `_call` method. When an error occurs, the `_call` method should when possible return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown then execution of the agent will stop.\n\n```typescript\nabstract class Tool {\n  abstract _call(arg: string): Promise<string>;\n\n  abstract name: string;\n\n  abstract description: string;\n}\n```\n\nAnother option is to create a tool on the fly using a `DynamicTool`. This is useful if you don't need the overhead of subclassing `Tool`.","metadata":{"id":84}}],["85",{"pageContent":"abstract name: string;\n\n  abstract description: string;\n}\n```\n\nAnother option is to create a tool on the fly using a `DynamicTool`. This is useful if you don't need the overhead of subclassing `Tool`.\nThe `DynamicTool` class takes as input a name, a description, and a function. Importantly, the name and the description will be used by the language model to determine when to call this function and with what parameters! So make sure to set these to some values the language model can reason about. The function provided is what will actually be called. When an error occurs, the function should, when possible, return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown then execution of the agent will stop.\n\nSee below for an example of defining and using `DynamicTool`s.\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport { initializeAgentExecutor } from \"langchain/agents\";\nimport { DynamicTool } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new DynamicTool({\n      name: \"FOO\",\n      description:\n        \"call this to get the value of foo. input should be an empty string.\",\n      func: () => \"baz\",\n    }),\n    new DynamicTool({\n      name: \"BAR\",\n      description:\n        \"call this to get the value of bar. input should be an empty string.\",\n      func: () => \"baz1\",\n    }),\n  ];\n\n  const executor = await initializeAgentExecutor(\n    tools,\n    model,\n    \"zero-shot-react-description\"\n  );\n\n  console.log(\"Loaded agent.\");\n\n  const input = `What is the value of foo?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```","metadata":{"id":85}}],["86",{"pageContent":"---\nsidebar_label: Integrations\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Integrations: Tools\n\nLangChain provides the following tools you can use out of the box:\n\n- `BingSerpAPI` - A wrapper around the Bing Search API. Useful for when you need to answer questions about current events. Input should be a search query.\n- `Calculator` - Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.\n- `IFTTTWebHook` - A wrapper around the IFTTT Webhook API. Useful for triggering IFTTT actions.\n- `JsonListKeys` and `JsonGetValue` - Useful for extracting data from JSON objects. These tools can be used collectively in a `JsonToolkit`.\n- `RequestsGet` and `RequestsPost` - Useful for making HTTP requests.\n- `SerpAPI` - A wrapper around the SerpAPI API. Useful for when you need to answer questions about current events. Input should be a search query.\n- `QuerySqlTool`, `InfoSqlTool`, `ListTablesSqlTool`, and `SqlCheckerTool` - Useful for interacting with SQL databases. Can be used together in a `SqlToolkit`.\n- `VectorStoreQATool` - Useful for retrieving relevant text data from a vector store.\n- `ZapierNLARunAction` - A wrapper around the Zapier NLP API. Useful for triggering Zapier actions with a natural language input. Best when used in a `ZapierToolkit`.","metadata":{"id":86}}],["87",{"pageContent":"# Agent with Zapier NLA Integration\n\nFull docs here: https://nla.zapier.com/api/v1/dynamic/docs\n\n**Zapier Natural Language Actions** gives you access to the 5k+ apps, 20k+ actions on Zapier's platform through a natural language API interface.\n\nNLA supports apps like Gmail, Salesforce, Trello, Slack, Asana, HubSpot, Google Sheets, Microsoft Teams, and thousands more apps: https://zapier.com/apps\n\nZapier NLA handles ALL the underlying API auth and translation from natural language --> underlying API call --> return simplified output for LLMs. The key idea is you, or your users, expose a set of actions via an oauth-like setup window, which you can then query and execute via a REST API.\n\nNLA offers both API Key and OAuth for signing NLA API requests.\n\nServer-side (API Key): for quickly getting started, testing, and production scenarios where LangChain will only use actions exposed in the developer's Zapier account (and will use the developer's connected accounts on Zapier.com)\n\nUser-facing (Oauth): for production scenarios where you are deploying an end-user facing application and LangChain needs access to end-user's exposed actions and connected accounts on Zapier.com\n\nThis quick start will focus on the server-side use case for brevity. Review full docs or reach out to nla@zapier.com for user-facing oauth developer support.\n\nThis example goes over how to use the Zapier integration an Agent. In code, below:\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport { initializeAgentExecutor, ZapierToolKit } from \"langchain/agents\";\nimport { ZapierNLAWrapper } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const zapier = new ZapierNLAWrapper();\n  const toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);\n\n  const executor = await initializeAgentExecutor(\n    toolkit.tools,\n    model,\n    \"zero-shot-react-description\",\n    true\n  );\n  console.log(\"Loaded agent.\");","metadata":{"id":87}}],["88",{"pageContent":"const executor = await initializeAgentExecutor(\n    toolkit.tools,\n    model,\n    \"zero-shot-react-description\",\n    true\n  );\n  console.log(\"Loaded agent.\");\n\n  const input = `Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```","metadata":{"id":88}}],["89",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport ConvoRetrievalQAExample from \"@examples/chains/conversational_qa.ts\";\n\n# `ConversationalRetrievalQAChain`\n\nThe `ConversationalRetrievalQA` chain builds on `RetrievalQAChain` to provide a chat history component.\n\nIt requires two inputs: a question and the chat history. It first combines the chat history and the question into a standalone question, then looks up relevant documents from the retriever, and then passes those documents and the question to a question answering chain to return a response.\n\nTo create one, you will need a retriever. In the below example, we will create one from a vectorstore, which can be created from embeddings.\nimport Example from \"@examples/chains/conversational_qa.ts\";\n\n<CodeBlock language=\"typescript\">{ConvoRetrievalQAExample}</CodeBlock>\n\nIn this code snippet, the fromLLM method of the `ConversationalRetrievalQAChain` class has the following signature:\n\n```typescript\nstatic fromLLM(\n  llm: BaseLanguageModel,\n  retriever: BaseRetriever,\n  options?: {\n    questionGeneratorTemplate?: string;\n    qaTemplate?: string;\n    returnSourceDocuments?: boolean;\n  }\n): ChatVectorDBQAChain\n```\n\nHere's an explanation of each of the attributes of the options object:\n\n- `questionGeneratorTemplate`: A string that specifies a question generation template. If provided, the `ConversationalRetrievalQAChain` will use this template to generate a question from the conversation context, instead of using the question provided in the question parameter. This can be useful if the original question does not contain enough information to retrieve a suitable answer.\n- `qaTemplate`: A string that specifies a response template. If provided, the `ConversationalRetrievalQAChain` will use this template to format a response before returning the result. This can be useful if you want to customize the way the response is presented to the end user.","metadata":{"id":89}}],["90",{"pageContent":"- `returnSourceDocuments`: A boolean value that indicates whether the `ConversationalRetrievalQAChain` should return the source documents that were used to retrieve the answer. If set to true, the documents will be included in the result returned by the call() method. This can be useful if you want to allow the user to see the sources used to generate the answer. If not set, the default value will be false.\n\nIn summary, the `questionGeneratorTemplate`, `qaTemplate`, and `returnSourceDocuments` options allow the user to customize the behavior of the `ConversationalRetrievalQAChain`","metadata":{"id":90}}],["91",{"pageContent":"import QAExample from \"@examples/chains/question_answering.ts\";\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Document QA Chains\n\nLangChain provides chains used for processing unstructured text data: `StuffDocumentsChain` and `MapReduceDocumentsChain`.\nThese chains are the building blocks more complex chains for processing unstructured text data and receive both documents and a question as input. They then utilize the language model to provide an answer to the question based on the given documents.\n\n<CodeBlock language=\"typescript\">{QAExample}</CodeBlock>","metadata":{"id":91}}],["92",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Index Related Chains\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Index Related Chains\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/index_related_chains)\n:::\n\nChains related to working with unstructured data stored in indexes.\n\n<DocCardList />","metadata":{"id":92}}],["93",{"pageContent":"import RetrievalQAExample from \"@examples/chains/retrieval_qa.ts\";\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# `RetrievalQAChain`\n\nThe `RetrievalQAChain` is a chain that combines a `Retriever` and a QA chain (described above). It is used to retrieve documents from a `Retriever` and then use a `QA` chain to answer a question based on the retrieved documents.\n\nIn the below example, we are using a `VectorStore` as the `Retriever`. By default, the `StuffDocumentsChain` is used as the `QA` chain.\n\n<CodeBlock language=\"typescript\">{RetrievalQAExample}</CodeBlock>","metadata":{"id":93}}],["94",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Chains\nsidebar_position: 6\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Chains\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains)\n:::info\n\nUsing a language model in isolation is fine for some applications, but it is often useful to combine language models with other sources of information, third-party APIs, or even other language models. This is where the concept of a chain comes in.\n\nLangChain provides a standard interface for chains, as well as a number of built-in chains that can be used out of the box. You can also create your own chains.\n\n<DocCardList />","metadata":{"id":94}}],["95",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: LLM Chain\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chains/llm_chain.ts\";\n\n# Getting Started: LLMChain\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/llm-chain)\n:::\n\nAn `LLMChain` is a simple chain that adds some functionality around language models. It is used widely throughout LangChain, including in other chains and agents.\n\nAn `LLMChain` consists of a `PromptTemplate` and a language model (either and LLM or chat model).\n\nWe can construct an LLMChain which takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":95}}],["96",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport AnalyzeDocumentExample from \"@examples/chains/analyze_document_chain_summarize.ts\";\n\n# `AnalyzeDocumentChain`\n\nYou can use the `AnalyzeDocumentChain`, which accepts a single piece of text as input and operates over it.\nThis chain takes care of splitting up the text and then passing it to the `MapReduceDocumentsChain` to generate a summary.\n\n<CodeBlock language=\"typescript\">{AnalyzeDocumentExample}</CodeBlock>","metadata":{"id":96}}],["97",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Other Chains\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Other Chains\n\nThis section highlights other examples of chains that exist.\n\n<DocCardList />","metadata":{"id":97}}],["98",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport SqlDBExample from \"@examples/chains/sql_db.ts\";\n\n# `SqlDatabaseChain`\n\nThe `SqlDatabaseChain` allows you to answer questions over a SQL database.\nThis example uses Chinook database, which is a sample database available for SQL Server, Oracle, MySQL, etc.\n\n## Set up\n\nFirst install `typeorm`:\n\n```bash npm2yarn\nnpm install typeorm\n```\n\nThen install the dependencies needed for your database. For example, for SQLite:\n\n```bash npm2yarn\nnpm install sqlite3\n```\n\nFor other databases see https://typeorm.io/#installation\n\nFinally follow the instructions on https://database.guide/2-sample-databases-sqlite/ to get the sample database for this example.\n\n<CodeBlock language=\"typescript\">{SqlDBExample}</CodeBlock>\n\nYou can include or exclude tables when creating the `SqlDatabase` object to help the chain focus on the tables you want.\nIt can also reduce the number of tokens used in the chain.\n\n```typescript\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n  includeTables: [\"Track\"],\n});\n```","metadata":{"id":98}}],["99",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\nimport SummarizeExample from \"@examples/chains/summarization_map_reduce.ts\";\n\n# Summarization\n\nA summarization chain can be used to summarize multiple documents. One way is to input multiple smaller documents, after they have been divided into chunks, and operate over them with a `MapReduceDocumentsChain`.\n\n<CodeBlock language=\"typescript\">{SummarizeExample}</CodeBlock>","metadata":{"id":99}}],["100",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Prompt Selectors\n---\n\n# Prompt Selectors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/chains/prompt-selector)\n:::\n\nOftentimes, you will want to programmatically select a prompt based on the type of model you are using in a chain. This is especially relevant when swapping chat models and LLMs.\n\nThe interface for prompt selectors is quite simple:\n\n```typescript\nabstract class BasePromptSelector {\n  abstract getPrompt(llm: BaseLanguageModel): BasePromptTemplate;\n}\n```\n\nThe `getPrompt` method takes in a language model and returns an appropriate prompt template.\n\nWe currently offer a `ConditionalPromptSelector` that allows you to specify a set of conditions and prompt templates. The first condition that evaluates to true will be used to select the prompt template.\n\n```typescript\nconst QA_PROMPT_SELECTOR = new ConditionalPromptSelector(DEFAULT_QA_PROMPT, [\n  [isChatModel, CHAT_PROMPT],\n]);\n```\n\nThis will return `DEFAULT_QA_PROMPT` if the model is not a chat model, and `CHAT_PROMPT` if it is.\n\nThe example below shows how to use a prompt selector when loading a chain:\n\n```typescript\nconst loadQAStuffChain = (\n  llm: BaseLanguageModel,\n  params: StuffQAChainParams = {}\n) => {\n  const { prompt = QA_PROMPT_SELECTOR.getPrompt(llm) } = params;\n  const llmChain = new LLMChain({ prompt, llm });\n  const chain = new StuffDocumentsChain({ llmChain });\n  return chain;\n};\n```","metadata":{"id":100}}],["101",{"pageContent":"# CSV files\n\nThis example goes over how to load data from CSV files. The second argument is the `column` name to extract from the CSV file. One document will be created for each row in the CSV file. When `column` is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's `pageContent`. When `column` is specified, one document is created for each row, and the value of the specified column is used as the document's pageContent.\n\n## Setup\n\n```bash npm2yarn\nnpm install d3-dsv\n```\n\n## Usage, extracting all columns\n\nExample CSV file:\n\n```csv\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\nExample code:\n\n```typescript\nimport { CSVLoader } from \"langchain/document_loaders\";\n\nconst loader = new CSVLoader(\"src/document_loaders/example_data/example.csv\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 1\ntext: This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 2\ntext: This is another sentence.\",\n  },\n]\n*/\n```\n\n## Usage, extracting a single column\n\nExample CSV file:\n\n```csv\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\nExample code:\n\n```typescript\nimport { CSVLoader } from \"langchain/document_loaders\";\n\nconst loader = new CSVLoader(\n  \"src/document_loaders/example_data/example.csv\",\n  \"text\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"id":101}}],["102",{"pageContent":"---\nsidebar_position: 1\nhide_table_of_contents: true\n---\n\n# Folders with multiple files\n\nThis example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.\n\nExample folder:\n\n```text\nsrc/document_loaders/example_data/example/\nâ”œâ”€â”€ example.json\nâ”œâ”€â”€ example.jsonl\nâ”œâ”€â”€ example.txt\nâ””â”€â”€ example.csv\n```\n\nExample code:\n\n```typescript\nimport {\n  DirectoryLoader,\n  JSONLoader,\n  JSONLinesLoader,\n  TextLoader,\n  CSVLoader,\n} from \"langchain/document_loaders\";\n\nconst loader = new DirectoryLoader(\n  \"src/document_loaders/example_data/example\",\n  {\n    \".json\": (path) => new JSONLoader(path, \"/texts\"),\n    \".jsonl\": (path) => new JSONLinesLoader(path, \"/html\"),\n    \".txt\": (path) => new TextLoader(path),\n    \".csv\": (path) => new CSVLoader(path, \"text\"),\n  }\n);\nconst docs = await loader.load();\nconsole.log({ docs });\n```","metadata":{"id":102}}],["103",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Docx files\n\nThis example goes over how to load data from docx files.\n\n# Setup\n\n```bash npm2yarn\nnpm install mammoth\n```\n\n# Usage\n\n```typescript\nimport { DocxLoader } from \"langchain/document_loaders\";\n\nconst loader = new DocxLoader(\n  \"src/document_loaders/tests/example_data/attention.docx\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"id":103}}],["104",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# EPUB files\n\nThis example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the `splitChapters` option to `false`.\n\n# Setup\n\n```bash npm2yarn\nnpm install epub2 html-to-text\n```\n\n# Usage, one document per chapter\n\n```typescript\nimport { EPubLoader } from \"langchain/document_loaders\";\n\nconst loader = new EPubLoader(\"src/document_loaders/example_data/example.epub\");\n\nconst docs = await loader.load();\n```\n\n# Usage, one document per file\n\n```typescript\nimport { EPubLoader } from \"langchain/document_loaders\";\n\nconst loader = new EPubLoader(\n  \"src/document_loaders/example_data/example.epub\",\n  {\n    splitChapters: false,\n  }\n);\n\nconst docs = await loader.load();\n```","metadata":{"id":104}}],["105",{"pageContent":"---\nlabel: \"File Loaders\"\nhide_table_of_contents: true\n---\n\n# File Loaders\n\nThese loaders are used to load files either given a file path or a Blob object.\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"id":105}}],["106",{"pageContent":"# JSON files\n\nThe JSON loader use [JSON pointer](https://github.com/janl/node-jsonpointer) to target keys in your JSON files you want to target.\n\n### No JSON pointer example\n\nThe most simple way of using it, is to specify no JSON pointer.\nThe loader will load all strings it finds in the JSON object.\n\nExample JSON file:\n\n```json\n{\n  \"texts\": [\"This is a sentence.\", \"This is another sentence.\"]\n}\n```\n\nExample code:\n\n```typescript\nimport { JSONLoader } from \"langchain/document_loaders\";\n\nconst loader = new JSONLoader(\"src/document_loaders/example_data/example.json\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```\n\n### Using JSON pointer example\n\nYou can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.\n\nIn this example, we want to only extract information from \"from\" and \"surname\" entries.\n\n```json\n{\n  \"1\": {\n    \"body\": \"BD 2023 SUMMER\",\n    \"from\": \"LinkedIn Job\",\n    \"labels\": [\"IMPORTANT\", \"CATEGORY_UPDATES\", \"INBOX\"]\n  },\n  \"2\": {\n    \"body\": \"Intern, Treasury and other roles are available\",\n    \"from\": \"LinkedIn Job2\",\n    \"labels\": [\"IMPORTANT\"],\n    \"other\": {\n      \"name\": \"plop\",\n      \"surname\": \"bob\"\n    }\n  }\n}\n```\n\nExample code:\n\n```typescript\nimport { JSONLoader } from \"langchain/document_loaders\";\n\nconst loader = new JSONLoader(\n  \"src/document_loaders/example_data/example.json\",\n  [\"/from\", \"/surname\"]\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"BD 2023 SUMMER\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"LinkedIn Job\",\n  },\n  ...\n]\n```","metadata":{"id":106}}],["107",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# JSONLines files\n\nThis example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.\n\nExample JSONLines file:\n\n```json\n{\"html\": \"This is a sentence.\"}\n{\"html\": \"This is another sentence.\"}\n```\n\nExample code:\n\n```typescript\nimport { JSONLinesLoader } from \"langchain/document_loaders\";\n\nconst loader = new JSONLinesLoader(\n  \"src/document_loaders/example_data/example.jsonl\",\n  \"/html\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"id":107}}],["108",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Notion markdown export\n\nThis example goes over how to load data from your Notion pages exported from the notion dashboard.\n\nFirst, export your notion pages as **Markdown & CSV** as per the offical explanation [here](https://www.notion.so/help/export-your-content). Make sure to select `include subpages` and `Create folders for subpages.`\n\nThen, unzip the downloaded file and move the unzipped folder into your repository. It should contain the markdown files of your pages.\n\nOnce the folder is in your repository, simply run the example below:\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/notion_markdown.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":108}}],["109",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# PDF files\n\nThis example goes over how to load data from PDF files. By default, one document will be created for each page in the PDF file, you can change this behavior by setting the `splitPages` option to `false`.\n\n# Setup\n\n```bash npm2yarn\nnpm install pdfjs-dist\n```\n\n# Usage, one document per page\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\");\n\nconst docs = await loader.load();\n```\n\n# Usage, one document per file\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  splitPages: false,\n});\n\nconst docs = await loader.load();\n```\n\n# Usage, legacy environments\n\nIn legacy environments, you can use the `pdfjs` option to provide a function that returns a promise that resolves to the `PDFJS` object. This is useful if you want to use a custom build of `pdfjs-dist` or if you want to use a different version of `pdfjs-dist`. Eg. here we use the legacy build of `pdfjs-dist`, which includes several polyfills that are not included in the default build.\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  pdfjs: () =>\n    import(\"pdfjs-dist/legacy/build/pdf.js\").then((mod) => mod.default),\n});\n```","metadata":{"id":109}}],["110",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Subtitles\n\nThis example goes over how to load data from subtitle files. One document will be created for each subtitles file.\n\n## Setup\n\n```bash npm2yarn\nnpm install srt-parser-2\n```\n\n## Usage\n\n```typescript\nimport { SRTLoader } from \"langchain/document_loaders\";\n\nconst loader = new SRTLoader(\n  \"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"id":110}}],["111",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Text files\n\nThis example goes over how to load data from text files.\n\n```typescript\nimport { TextLoader } from \"langchain/document_loaders\";\n\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\n\nconst docs = await loader.load();\n```","metadata":{"id":111}}],["112",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Unstructured\n\nThis example covers how to use [Unstructured](https://www.unstructured.io) to load files of many types. Unstructured currently supports loading of text files, powerpoints, html, pdfs, images, and more.\n\n## Setup\n\nYou can run Unstructured locally in your computer using Docker. To do so, you need to have Docker installed. You can find the instructions to install Docker [here](https://docs.docker.com/get-docker/).\n\n```bash\ndocker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0\n```\n\n## Usage\n\nOnce Unstructured is running, you can use it to load files from your computer. You can use the following code to load a file from your computer.\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/unstructured.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":112}}],["113",{"pageContent":"---\nsidebar_label: Examples\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Document Loaders\n\n<DocCardList />","metadata":{"id":113}}],["114",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# College Confidential\n\nThis example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { CollegeConfidentialLoader } from \"langchain/document_loaders\";\n\nconst loader = new CollegeConfidentialLoader(\n  \"https://www.collegeconfidential.com/colleges/brown-university/\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"id":114}}],["115",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# GitBook\n\nThis example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Load from single GitBook page\n\n```typescript\nimport { GitbookLoader } from \"langchain/document_loaders\";\n\nconst loader = new GitbookLoader(\n  \"https://docs.gitbook.com/product-tour/navigation\"\n);\n\nconst docs = await loader.load();\n```\n\n## Load from all paths in a given GitBook\n\nFor this to work, the GitbookLoader needs to be initialized with the root path (https://docs.gitbook.com in this example) and have `shouldLoadAllPaths` set to `true`.\n\n```typescript\nimport { GitbookLoader } from \"langchain/document_loaders\";\n\nconst loader = new GitbookLoader(\"https://docs.gitbook.com\", {\n  shouldLoadAllPaths: true,\n});\n\nconst docs = await loader.load();\n```","metadata":{"id":115}}],["116",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# GitHub\n\nThis example goes over how to load data from a GitHub repository.\nYou can set the `GITHUB_ACCESS_TOKEN` environment variable to a GitHub access token to increase the rate limit and access private repositories.\n\n```typescript\nimport { GithubRepoLoader } from \"langchain/document_loaders\";\n\nconst loader = new GithubRepoLoader(\n  \"https://github.com/hwchase17/langchainjs\",\n  { branch: \"main\", recursive: false, unknown: \"warn\" }\n);\nconst docs = await loader.load();\n```","metadata":{"id":116}}],["117",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Hacker News\n\nThis example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { HNLoader } from \"langchain/document_loaders\";\n\nconst loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\n\nconst docs = await loader.load();\n```","metadata":{"id":117}}],["118",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# IMSDB\n\nThis example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { IMSDBLoader } from \"langchain/document_loaders\";\n\nconst loader = new IMSDBLoader(\"https://imsdb.com/scripts/BlacKkKlansman.html\");\n\nconst docs = await loader.load();\n```","metadata":{"id":118}}],["119",{"pageContent":"---\nlabel: \"Web Loaders\"\nhide_table_of_contents: true\n---\n\n# Web Loaders\n\nThese loaders are used to load web resources.\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"id":119}}],["120",{"pageContent":"---\nsidebar_position: 1\nhide_table_of_contents: true\n---\n\n# Webpages, with Cheerio\n\nThis example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.\n\nCheerio is a fast and lightweight library that allows you to parse and traverse HTML documents using a jQuery-like syntax. You can use Cheerio in Node.js to extract data from web pages, without having to render them in a browser.\n\nHowever, Cheerio does not simulate a web browser, so it cannot execute JavaScript code on the page. This means that it cannot extract data from dynamic web pages that require JavaScript to render. To do that, you can use the [PuppeteerWebBaseLoader](./web_puppeteer.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { CheerioWebBaseLoader } from \"langchain/document_loaders\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"id":120}}],["121",{"pageContent":"---\nsidebar_position: 2\nhide_table_of_contents: true\n---\n\n# Webpages, with Puppeteer\n\nThis example goes over how to load data from webpages using Puppeteer. One document will be created for each webpage.\n\nPuppeteer is a Node.js library that provides a high-level API for controlling headless Chrome or Chromium. You can use Puppeteer to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.\n\nIf you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [CheerioWebBaseLoader](./web_cheerio.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install puppeteer\n```\n\n## Usage\n\n```typescript\nimport { PuppeteerWebBaseLoader } from \"langchain/document_loaders\";\n\n/**\n * Loader uses `page.evaluate(() => document.body.innerHTML)`\n * as default evaluate function\n **/\nconst loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\");\n\nconst docs = await loader.load();\n```\n\n## Options\n\nHere's an explanation of the parameters you can pass to the PuppeteerWebBaseLoader constructor using the PuppeteerWebBaseLoaderOptions interface:\n\n```typescript\ntype PuppeteerWebBaseLoaderOptions = {\n  launchOptions?: PuppeteerLaunchOptions;\n  gotoOptions?: PuppeteerGotoOptions;\n  evaluate?: (page: Page, browser: Browser) => Promise<string>;\n};\n```\n\n1. `launchOptions`: an optional object that specifies additional options to pass to the puppeteer.launch() method. This can include options such as the headless flag to launch the browser in headless mode, or the slowMo option to slow down Puppeteer's actions to make them easier to follow.\n\n2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.","metadata":{"id":121}}],["122",{"pageContent":"3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using the page.evaluate() method. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.\n\nBy passing these options to the `PuppeteerWebBaseLoader` constructor, you can customize the behavior of the loader and use Puppeteer's powerful features to scrape and interact with web pages.\n\nHere is a basic example to do it:\n\n```typescript\nimport { PuppeteerWebBaseLoader } from \"langchain/document_loaders\";\n\nconst loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n  /** Pass custom evaluate, in this case you get page and browser instances */\n  async evaluate(page: Page, browser: Browser) {\n    await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n    const result = await page.evaluate(() => document.body.innerHTML);\n    return result;\n  },\n});\n\nconst docs = await loader.load();\n```","metadata":{"id":122}}],["123",{"pageContent":"---\nsidebar_label: Document Loaders\nsidebar_position: 1\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Document Loaders\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/document-loaders)\n:::\n\nDocument loaders make it easy to create [Documents](../../schema/document.md) from a variety of sources. These documents can then be loaded onto [Vector Stores](../vector_stores/) to load documents from a source.\n\n```typescript\ninterface DocumentLoader {\n  load(): Promise<Document[]>;\n\n  loadAndSplit(textSplitter?: TextSplitter): Promise<Document[]>;\n}\n```\n\nDocument Loaders expose two methods, `load` and `loadAndSplit`. `load` will load the documents from the source and return them as an array of [Documents](../../schema/document.md). `loadAndSplit` will load the documents from the source, split them using the provided [TextSplitter](../text_splitters/index.mdx), and return them as an array of [Documents](../../schema/document.md).\n\n## All Document Loaders\n\n<DocCardList />\n\n## Advanced\n\nIf you want to implement your own Document Loader, you have a few options.\n\n### Subclassing `BaseDocumentLoader`\n\nYou can extend the `BaseDocumentLoader` class directly. The `BaseDocumentLoader` class provides a few convenience methods for loading documents from a variety of sources.\n\n```typescript\nabstract class BaseDocumentLoader implements DocumentLoader {\n  abstract load(): Promise<Document[]>;\n}\n```\n\n### Subclassing `TextLoader`\n\nIf you want to load documents from a text file, you can extend the `TextLoader` class. The `TextLoader` class takes care of reading the file, so all you have to do is implement a parse method.\n\n```typescript\nabstract class TextLoader extends BaseDocumentLoader {\n  abstract parse(raw: string): Promise<string[]>;\n}\n```\n\n### Subclassing `BufferLoader`\n\nIf you want to load documents from a binary file, you can extend the `BufferLoader` class. The `BufferLoader` class takes care of reading the file, so all you have to do is implement a parse method.\n\n```typescript","metadata":{"id":123}}],["124",{"pageContent":"```typescript\nabstract class BufferLoader extends BaseDocumentLoader {\n  abstract parse(\n    raw: Buffer,\n    metadata: Document[\"metadata\"]\n  ): Promise<Document[]>;\n}\n```","metadata":{"id":124}}],["125",{"pageContent":"---\nsidebar_position: 4\nhide_table_of_contents: true\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Indexes\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing)\n:::\n\nThis section deals with everything related to bringing your own data into LangChain, indexing it, and making it available for LLMs/Chat Models.\n\n<DocCardList />","metadata":{"id":125}}],["126",{"pageContent":"# ChatGPT Plugin Retriever\n\nThis example shows how to use the ChatGPT Retriever Plugin within LangChain.\n\nTo set up the ChatGPT Retriever Plugin, please follow instructions [here](https://github.com/openai/chatgpt-retrieval-plugin).\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/chatgpt-plugin.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":126}}],["127",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 4\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Retrievers\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/retriever)\n:::\n\nA way of storing data such that it can be queried by a language model. The only interface this object must expose is a `getRelevantDocuments` method which takes in a string query and returns a list of Documents.\n\n<DocCardList />","metadata":{"id":127}}],["128",{"pageContent":"# Remote Retriever\n\nThis example shows how to use the Metal Retriever in a `RetrievalQAChain` to retrieve documents from Metal index.\n\n## Setup\n\n```bash npm2yarn\nnpm i @getmetal/metal-sdk\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/metal.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":128}}],["129",{"pageContent":"# Remote Retriever\n\nThis example shows how to use a Remote Retriever in a `RetrievalQAChain` to retrieve documents from a remote server.\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chains/retrieval_qa_with_remote.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":129}}],["130",{"pageContent":"# Supabase Hybrid Search\n\nLangchain supports hybrid search with a Supabase Postgres database. The hybrid search combines the postgres `pgvector` extension (similarity search) and Full-Text Search (keyword search) to retrieve documents. You can add documents via SupabaseVectorStore `addDocuments` function. SupabaseHybridKeyWordSearch accepts embedding, supabase client, number of results for similarity search, and number of results for keyword search as parameters. The `getRelevantDocuments` function produces a list of documents that has duplicates removed and is sorted by relevance score.\n\n## Setup\n\n### Install the library with\n\n```bash npm2yarn\nnpm install -S @supabase/supabase-js\n```\n\n### Create a table and search functions in your database\n\nRun this in your database:\n\n```sql\n-- Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n\n-- Create a table to store your documents\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);\n\n-- Create a function to similarity search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n\n-- Create an index to be used by the similarity search function\ncreate index on documents\n  using ivfflat (embedding vector_cosine_ops)\n  with (lists = 100);\n\n-- Create a function to keyword search for documents\ncreate function kw_match_documents(query_text text, match_count int)\nreturns table (id bigint, content text, metadata jsonb, similarity real)\nas $$\n\nbegin\nreturn query execute","metadata":{"id":130}}],["131",{"pageContent":"create function kw_match_documents(query_text text, match_count int)\nreturns table (id bigint, content text, metadata jsonb, similarity real)\nas $$\n\nbegin\nreturn query execute\nformat('select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity\nfrom documents\nwhere to_tsvector(content) @@ plainto_tsquery($1)\norder by similarity desc\nlimit $2')\nusing query_text, match_count;\nend;\n$$ language plpgsql;\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/supabase_hybrid.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":131}}],["132",{"pageContent":"# VectorStore\n\nThe main supported type of Retriever is one backed by a Vector Store.\nOnce you've created a Vector Store, the way to use it as a Retriever is very simple:\n\n```typescript\nvectorStore = ...\nretriever = vectorStore.asRetriever()\n```","metadata":{"id":132}}],["133",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# CharacterTextSplitter\n\nBesides the `RecursiveCharacterTextSplitter`, there is also the more standard `CharacterTextSplitter`. This splits only on one type of character (defaults to `\"\\n\\n\"`). You can use it in the exact same way.\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { CharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = \"foo bar baz 123\";\nconst splitter = new CharacterTextSplitter({\n  separator: \" \",\n  chunkSize: 7,\n  chunkOverlap: 3,\n});\nconst output = await splitter.createDocuments([text]);\n```","metadata":{"id":133}}],["134",{"pageContent":"---\nsidebar_label: Examples\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Text Splitters: Examples\n\n<DocCardList />","metadata":{"id":134}}],["135",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# `MarkdownTextSplitter`\n\nIf your content is in Markdown format then `MarkdownTextSplitter`. This class will split your content into documents based on the Markdown headers. For example, if you have the following Markdown content:\n\n```markdown\n# Header 1\n\nThis is some content.\n\n## Header 2\n\nThis is some more content.\n\n# Header 3\n\nThis is even more content.\n```\n\nThen the `MarkdownTextSplitter` will split the content into three documents:\n\n```typescript\nimport { MarkdownTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `# Header 1\n\nThis is some content.\n\n## Header 2\n\nThis is some more content.\n\n# Header 3\n\nThis is even more content.`;\n\nconst splitter = new MarkdownTextSplitter();\n\nconst output = await splitter.createDocuments([text], {\n  metadata: \"something\",\n});\n/*\n[\n  {\n    \"pageContent\": \"# Header 1\\n\\nThis is some content.\",\n    \"metadata\": \"something\"\n  },\n  {\n    \"pageContent\": \"## Header 2\\n\\nThis is some more content.\",\n    \"metadata\": \"something\"\n  },\n  {\n    \"pageContent\": \"# Header 3\\n\\nThis is even more content.\",\n    \"metadata\": \"something\"\n  }\n]\n*/\n```","metadata":{"id":135}}],["136",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# `RecursiveCharacterTextSplitter`\n\nThe recommended TextSplitter is the `RecursiveCharacterTextSplitter`. This will split documents recursively by different characters - starting with `\"\\n\\n\"`, then `\"\\n\"`, then `\" \"`. This is nice because it will try to keep all the semantically relevant content in the same place for as long as possible.\n\nImportant parameters to know here are `chunkSize` and `chunkOverlap`. `chunkSize` controls the max size (in terms of number of characters) of the final documents. `chunkOverlap` specifies how much overlap there should be between chunks. This is often helpful to make sure that the text isn't split weirdly. In the example below we set these values to be small (for illustration purposes), but in practice they default to `4000` and `200` respectively.\n\n```typescript\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 1,\n});\n\nconst output = await splitter.createDocuments([text]);\n```\n\nYou'll note that in the above example we are splitting a raw text string and getting back a list of documents. We can also split documents directly.\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 1,\n});\n\nconst docOutput = await splitter.splitDocuments([\n  new Document({ pageContent: text }),\n]);\n```","metadata":{"id":136}}],["137",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# TokenTextSplitter\n\nFinally, `TokenTextSplitter` splits a raw text string by first converting the text into BPE tokens, then split these tokens into chunks and convert the tokens within a single chunk back into text.\n\nTo utilize the `TokenTextSplitter`, first install the accompanying required library\n\n```bash npm2yarn\nnpm install -S @dqbd/tiktoken\n```\n\nThen, you can use it like so:\n\n```typescript\nimport { Document } from \"langchain/document\";\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\n\nconst text = \"foo bar baz 123\";\n\nconst splitter = new TokenTextSplitter({\n  encodingName: \"gpt2\",\n  chunkSize: 10,\n  chunkOverlap: 0,\n});\n\nconst output = await splitter.createDocuments([text]);\n```","metadata":{"id":137}}],["138",{"pageContent":"---\nsidebar_label: Text Splitters\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Text Splitters\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/text-splitters)\n:::\n\nLanguage Models are often limited by the amount of text that you can pass to them. Therefore, it is neccessary to split them up into smaller chunks. LangChain provides several utilities for doing so.\n\nUsing a Text Splitter can also help improve the results from vector store searches, as eg. smaller chunks may sometimes be more likely to match a query. Testing different chunk sizes (and chunk overlap) is a worthwhile exercise to tailor the results to your use case.\n\n```typescript\ninterface TextSplitter {\n  chunkSize: number;\n\n  chunkOverlap: number;\n\n  createDocuments(\n    texts: string[],\n    metadatas?: Record<string, any>[]\n  ): Promise<Document[]>;\n\n  splitDocuments(documents: Document[]): Promise<Document[]>;\n}\n```\n\nText Splitters expose two methods, `createDocuments` and `splitDocuments`. The former takes a list of raw text strings and returns a list of documents. The latter takes a list of documents and returns a list of documents. The difference is that `createDocuments` will split the raw text strings into chunks, while `splitDocuments` will split the documents into chunks.\n\n## All Text Splitters\n\n<DocCardList />\n\n## Advanced\n\nIf you want to implement your own custom Text Splitter, you only need to subclass TextSplitter and implement a single method `splitText`. The method takes a string and returns a list of strings. The returned strings will be used as the chunks.\n\n```typescript\nabstract class TextSplitter {\n  abstract splitText(text: string): Promise<string[]>;\n}\n```","metadata":{"id":138}}],["139",{"pageContent":"---\nsidebar_label: \"Vector Stores\"\nsidebar_position: 3\n---\n\n# Getting Started: Vector Stores\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/indexing/vectorstore)\n:::\n\nA vector store is a particular type of database optimized for storing documents and their [embeddings](../../models/embeddings/), and then fetching of the most relevant documents for a particular query, ie. those whose embeddings are most similar to the embedding of the query.\n\n```typescript\ninterface VectorStore {\n  /**\n   * Add more documents to an existing VectorStore\n   */\n  addDocuments(documents: Document[]): Promise<void>;\n\n  /**\n   * Search for the most similar documents to a query\n   */\n  similaritySearch(\n    query: string,\n    k?: number,\n    filter?: object | undefined\n  ): Promise<Document[]>;\n\n  /**\n   * Search for the most similar documents to a query,\n   * and return their similarity score\n   */\n  similaritySearchWithScore(\n    query: string,\n    k = 4,\n    filter: object | undefined = undefined\n  ): Promise<[object, number][]>;\n\n  /**\n   * Turn a VectorStore into a Retriever\n   */\n  asRetriever(k?: number): BaseRetriever;\n\n  /**\n   * Advanced: Add more documents to an existing VectorStore,\n   * when you already have their embeddings\n   */\n  addVectors(vectors: number[][], documents: Document[]): Promise<void>;\n\n  /**\n   * Advanced: Search for the most similar documents to a query,\n   * when you already have the embedding of the query\n   */\n  similaritySearchVectorWithScore(\n    query: number[],\n    k: number,\n    filter?: object\n  ): Promise<[Document, number][]>;\n}\n```\n\nYou can create a vector store from a list of [Documents](../../schema/document), or from a list of texts and their corresponding metadata. You can also create a vector store from an existing index, the signature of this method depends on the vector store you're using, check the documentation of the vector store you're interested in.\n\n```typescript\nabstract class BaseVectorStore implements VectorStore {\n  static fromTexts(\n    texts: string[],","metadata":{"id":139}}],["140",{"pageContent":"```typescript\nabstract class BaseVectorStore implements VectorStore {\n  static fromTexts(\n    texts: string[],\n    metadatas: object[] | object,\n    embeddings: Embeddings,\n    dbConfig: Record<string, any>\n  ): Promise<VectorStore>;\n\n  static fromDocuments(\n    docs: Document[],\n    embeddings: Embeddings,\n    dbConfig: Record<string, any>\n  ): Promise<VectorStore>;\n}\n```\n\n## Which one to pick?\n\nHere's a quick guide to help you pick the right vector store for your use case:\n\n- If you're after something that can just run inside your application, in-memory, without any other servers to stand up, then go for [HNSWLib](./integrations/hnswlib)\n- If you come from Python and you were looking for something similar to FAISS, pick [HNSWLib](./integrations/hnswlib)\n- If you're looking for an open-source full-featured vector database that you can run locally in a docker container, then go for [Chroma](./integrations/chroma)\n- If you're using Supabase already then look at the [Supabase](./integrations/supabase) vector store to use the same Postgres database for your embeddings too\n- If you're looking for a production-ready vector store you don't have to worry about hosting yourself, then go for [Pinecone](./integrations/pinecone)\n\n## All Vector Stores\n\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />","metadata":{"id":140}}],["141",{"pageContent":"# Chroma\n\nChroma is an open-source Apache 2.0 embedding database.\n\nUse [chroma](https://github.com/chroma-core/chroma) with langchainjs.\n\n## Setup\n\n1. Run chroma inside of docker on your computer [docs](https://docs.trychroma.com/api-reference)\n2. Install the chroma js client.\n\n```bash npm2yarn\nnpm install -S chromadb\n```\n\n## Index and query docs\n\n```typescript\nimport { Chroma } from \"langchain/vectorstores\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\n\n// text sample from Godel, Escher, Bach\nconst vectorStore = await Chroma.fromTexts(\n  [\n    \"Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\\\n        Harmonic Labyrinth of the dreaded Majotaur?\",\n    \"Achilles: Yiikes! What is that?\",\n    \"Tortoise: They say-although I person never believed it myself-that an I\\\n        Majotaur has created a tiny labyrinth sits in a pit in the middle of\\\n        it, waiting innocent victims to get lost in its fears complexity.\\\n        Then, when they wander and dazed into the center, he laughs and\\\n        laughs at them-so hard, that he laughs them to death!\",\n    \"Achilles: Oh, no!\",\n    \"Tortoise: But it's only a myth. Courage, Achilles.\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel-escher-bach\",\n  }\n);\n\n// or alternatively from docs\nconst vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {\n  collectionName: \"goldel-escher-bach\",\n});\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```\n\n## Query docs from existing collection\n\n```typescript\nimport { Chroma } from \"langchain/vectorstores\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\n\nconst vectorStore = await Chroma.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel-escher-bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```","metadata":{"id":141}}],["142",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# HNSWLib\n\nHNSWLib is an in-memory vectorstore that can be saved to a file. It uses [HNSWLib](https://github.com/nmslib/hnswlib).\n\n## Setup\n\nYou can install it with\n\n```bash npm2yarn\nnpm install hnswlib-node\n```\n\n## Usage\n\n### Create a new index from texts\n\nimport ExampleTexts from \"@examples/indexes/vector_stores/hnswlib.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleTexts}</CodeBlock>\n\n### Create a new index from a loader\n\nimport ExampleLoader from \"@examples/indexes/vector_stores/hnswlib_fromdocs.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleLoader}</CodeBlock>\n\n### Save an index to a file and load it again\n\nimport ExampleSave from \"@examples/indexes/vector_stores/hnswlib_saveload.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleSave}</CodeBlock>","metadata":{"id":142}}],["143",{"pageContent":"---\nsidebar_label: Integrations\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Vector Stores: Integrations\n\n<DocCardList />","metadata":{"id":143}}],["144",{"pageContent":"# Pinecone\n\nLangchain.js accepts [@pinecone-database/pinecone](https://docs.pinecone.io/docs/node-client) as the client for Pinecone vectorstore. Install the library with\n\n```bash npm2yarn\nnpm install -S dotenv langchain @pinecone-database/pinecone\n```\n\n## Index docs\n\n```typescript\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport * as dotenv from \"dotenv\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\nimport { PineconeStore } from \"langchain/vectorstores\";\n\ndotenv.config();\n\nconst client = new PineconeClient();\nawait client.init({\n  apiKey: process.env.PINECONE_API_KEY,\n  environment: process.env.PINECONE_ENVIRONMENT,\n});\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX);\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"pinecone is a vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"pinecones are the woody fruiting body and of a pine tree\",\n  }),\n];\n\nawait PineconeStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n  pineconeIndex,\n});\n```\n\n## Query docs\n\n```typescript\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport * as dotenv from \"dotenv\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\nimport { OpenAI } from \"langchain/llms\";\nimport { PineconeStore } from \"langchain/vectorstores\";\n\ndotenv.config();\n\nconst client = new PineconeClient();\nawait client.init({\n  apiKey: process.env.PINECONE_API_KEY,\n  environment: process.env.PINECONE_ENVIRONMENT,\n});\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX);\n\nconst vectorStore = await PineconeStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  { pineconeIndex }\n);\n\n/* Search the vector DB independently with meta filters */","metadata":{"id":144}}],["145",{"pageContent":"const vectorStore = await PineconeStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  { pineconeIndex }\n);\n\n/* Search the vector DB independently with meta filters */\nconst results = await vectorStore.similaritySearch(\"pinecone\", 1, {\n  foo: \"bar\",\n});\nconsole.log(results);\n/*\n[\n  Document {\n    pageContent: 'pinecone is a vector db',\n    metadata: { foo: 'bar' }\n  }\n]\n*/\n\n/* Use as part of a chain (currently no metadata filters) */\nconst model = new OpenAI();\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n  k: 1,\n  returnSourceDocuments: true,\n});\nconst response = await chain.call({ query: \"What is pinecone?\" });\nconsole.log(response);\n/*\n{\n  text: ' A pinecone is the woody fruiting body of a pine tree.',\n  sourceDocuments: [\n    Document {\n      pageContent: 'pinecones are the woody fruiting body and of a pine tree',\n      metadata: [Object]\n    }\n  ]\n}\n*/\n```","metadata":{"id":145}}],["146",{"pageContent":"# Supabase\n\nLangchain supports using Supabase Postgres database as a vector store, using the `pgvector` postgres extension. Refer to the [Supabase blog post](https://supabase.com/blog/openai-embeddings-postgres-vector) for more information.\n\n## Setup\n\n### Install the library with\n\n```bash npm2yarn\nnpm install -S @supabase/supabase-js\n```\n\n### Create a table and search function in your database\n\nRun this in your database:\n\n```sql\n-- Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n\n-- Create a table to store your documents\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);\n\n-- Create a function to search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n\n-- Create an index to be used by the search function\ncreate index on documents\n  using ivfflat (embedding vector_cosine_ops)\n  with (lists = 100);\n```\n\n## Usage\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/indexes/vector_stores/supabase.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":146}}],["147",{"pageContent":"---\nhide_table_of_contents: true\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/chat/memory.ts\";\n\n# Using Buffer Memory with Chat Models\n\nThis example covers how to use chat-specific memory classes with chat models.\nThe key thing to notice is that setting `returnMessages: true` makes the memory return a list of chat messages instead of a string.\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>","metadata":{"id":147}}],["148",{"pageContent":"# Buffer Memory\n\nBufferMemory is the simplest type of memory - it just remembers previous conversational back and forths directly.\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferMemory();\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```\n\nYou can also load messages into a `BufferMemory` instance by creating and passing in a `ChatHistory` object.\nThis lets you easily pick up state from past conversations:\n\n```typescript\nimport { ChatMessageHistory } from \"langchain/memory\";\nimport { HumanChatMessage, AIChatMessage } from \"langchain/schema\";\n\nconst pastMessages = [\n  new HumanChatMessage(\"My name's Jonas\"),\n  new AIChatMessage(\"Nice to meet you, Jonas!\"),\n];\n\nconst memory = new BufferMemory({\n  chatHistory: new ChatMessageHistory(pastMessages),\n});\n```","metadata":{"id":148}}],["149",{"pageContent":"# Buffer Window Memory\n\nBufferWindowMemory keeps track of the back-and-forths in conversation, and then uses a window of size `k` to surface the last `k` back-and-forths to use as memory.\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\nimport { BufferWindowMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferWindowMemory({ k: 1 });\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```","metadata":{"id":149}}],["150",{"pageContent":"---\nsidebar_label: Examples\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Examples: Memory\n\n<DocCardList />","metadata":{"id":150}}],["151",{"pageContent":"---\nsidebar_label: Memory\nsidebar_position: 5\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Memory\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/memory)\n:::\n\nMemory is the concept of storing and retrieving data in the process of a conversation. There are two main methods, `loadMemoryVariables` and `saveContext`. The first method is used to retrieve data from memory (optionally using the current input values), and the second method is used to store data in memory.\n\n```typescript\nexport type InputValues = Record<string, any>;\n\nexport type OutputValues = Record<string, any>;\n\ninterface BaseMemory {\n  loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n\n  saveContext(\n    inputValues: InputValues,\n    outputValues: OutputValues\n  ): Promise<void>;\n}\n```\n\n:::note\nDo not share the same memory instance between two different chains, a memory instance represents the history of a single conversation\n:::\n\n:::note\nIf you deploy your LangChain app on a serverless environment do not store memory instances in a variable, as your hosting provider may have reset it by the next time the function is called.\n:::\n\n## All Memory classes\n\n<DocCardList />\n\n## Advanced\n\nTo implement your own memory class you have two options:\n\n### Subclassing `BaseChatMemory`\n\nThis is the easiest way to implement your own memory class. You can subclass `BaseChatMemory`, which takes care of `saveContext` by saving inputs and outputs as [Chat Messages](../schema/chat-messages.md), and implement only the `loadMemoryVariables` method. This method is responsible for returning the memory variables that are relevant for the current input values.\n\n```typescript\nabstract class BaseChatMemory extends BaseMemory {\n  chatHistory: ChatMessageHistory;\n\n  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n}\n```\n\n### Subclassing `BaseMemory`","metadata":{"id":151}}],["152",{"pageContent":"abstract class BaseChatMemory extends BaseMemory {\n  chatHistory: ChatMessageHistory;\n\n  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n}\n```\n\n### Subclassing `BaseMemory`\n\nIf you want to implement a more custom memory class, you can subclass `BaseMemory` and implement both `loadMemoryVariables` and `saveContext` methods. The `saveContext` method is responsible for storing the input and output values in memory. The `loadMemoryVariables` method is responsible for returning the memory variables that are relevant for the current input values.\n\n```typescript\nabstract class BaseMemory {\n  abstract loadMemoryVariables(values: InputValues): Promise<MemoryVariables>;\n\n  abstract saveContext(\n    inputValues: InputValues,\n    outputValues: OutputValues\n  ): Promise<void>;\n}\n```","metadata":{"id":152}}],["153",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/chat/chat.ts\";\nimport StreamingExample from \"@examples/models/chat/chat_streaming.ts\";\nimport TimeoutExample from \"@examples/models/chat/chat_timeout.ts\";\n\n# Additional Functionality: Chat Models\n\nWe offer a number of additional features for chat models. In the examples below, we'll be using the `ChatOpenAI` model.\n\n## Additional Methods\n\nLangChain provides a number of additional methods for interacting with chat models:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Streaming\n\nSimilar to LLMs, you can stream responses from a chat model. This is useful for chatbots that need to respond to user input in real-time.\n\n<CodeBlock language=\"typescript\">{StreamingExample}</CodeBlock>\n\n## Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.\n\n## Dealing with Rate Limits\n\nSome providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating a Chat Model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.","metadata":{"id":153}}],["154",{"pageContent":"To use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models\";\n\nconst model = new ChatOpenAI({ maxConcurrency: 5 });\n```\n\n## Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models\";\n\nconst model = new ChatOpenAI({ maxRetries: 10 });\n```","metadata":{"id":154}}],["155",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Chat Models\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/chat/chat_quick_start.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Chat Models\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/chat-model)\n:::\n\nLangChain provides a standard interface for using chat models. Chat models are a variation on language models.\nWhile chat models use language models under the hood, the interface they expose is a bit different.\nRather than expose a \"text in, text out\" API, they expose an interface where \"chat messages\" are the inputs and outputs.\n\n## Chat Messages\n\nA `ChatMessage` is what we refer to as the modular unit of information for a chat model.\nAt the moment, this consists of a `\"text\"` field, which refers to the content of the chat message.\n\nThere are currently four different classes of `ChatMessage` supported by LangChain:\n\n- `HumanChatMessage`: A chat message that is sent as if from a Human's point of view.\n- `AIChatMessage`: A chat message that is sent from the point of view of the AI system to which the Human is corresponding.\n- `SystemChatMessage`: A chat message that gives the AI system some information about the conversation. This is usually sent at the beginning of a conversation.\n- `ChatMessage`: A generic chat message, with not only a `\"text\"` field but also an arbitrary `\"role\"` field.\n\n> **_Note:_** Currently, the only chat-based model we support is `ChatOpenAI` (with gpt-4 and gpt-3.5-turbo), but anticipate adding more in the future.\n\nTo get started, simply use the `call` method of an `LLM` implementation, passing in a `string` input. In this example, we are using the `ChatOpenAI` implementation:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Dig deeper\n\n<DocCardList />","metadata":{"id":155}}],["156",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\n# Integrations: Chat Models\n\nLangChain offers a number of Chat Models implementations that integrate with various model providers. These are:\n\n## `OpenAI`\n\n```typescript\nimport { ChatOpenAI } from \"langchain/chat_models\";\n\n// Expects an OpenAI API key to be set in the env variable OPENAI_API_KEY\nconst model = new ChatOpenAI({ temperature: 0.9 });\n```\n\n## `Anthropic`\n\n```typescript\nimport { ChatAnthropic } from \"langchain/chat_models\";\n\n// Expects an Anthropic API key to be set in the env variable ANTHROPIC_API_KEY\nconst model = new ChatAnthropic({ temperature: 0.9 });\n```","metadata":{"id":156}}],["157",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport TimeoutExample from \"@examples/models/embeddings/openai_timeout.ts\";\n\n# Additional Functionality: Embeddings\n\nWe offer a number of additional features for chat models. In the examples below, we'll be using the `ChatOpenAI` model.\n\n## Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.\n\n## Dealing with Rate Limits\n\nSome providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating an Embeddings model. This option allows you to specify the maximum number of concurrent requests you want to make to the provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.\n\nTo use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\n\nconst model = new OpenAIEmbeddings({ maxConcurrency: 5 });\n```\n\n## Dealing with API Errors","metadata":{"id":157}}],["158",{"pageContent":"```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\n\nconst model = new OpenAIEmbeddings({ maxConcurrency: 5 });\n```\n\n## Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\n\nconst model = new OpenAIEmbeddings({ maxRetries: 10 });\n```","metadata":{"id":158}}],["159",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Embeddings\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: Embeddings\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/text-embedding-model)\n:::\n\nEmbeddings can be used to create a numerical representation of textual data. This numerical representation is useful because it can be used to find similar documents.\n\nBelow is an example of how to use the OpenAI embeddings. Embeddings occasionally have different embedding methods for queries versus documents, so the embedding class exposes a `embedQuery` and `embedDocuments` method.\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\n\n/* Create instance */\nconst embeddings = new OpenAIEmbeddings();\n\n/* Embed queries */\nconst res = await embeddings.embedQuery(\"Hello world\");\n/*\n[\n   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,\n    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,\n    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,\n   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,\n   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,\n  -0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,\n   0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,\n   -0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,\n    0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,\n    0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,\n    0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,\n    0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,\n    0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,\n      0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,\n    0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,","metadata":{"id":159}}],["160",{"pageContent":"0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,\n    0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,\n   0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,\n    0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,\n   0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,\n    0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,\n  -0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,\n  ... 1436 more items\n]\n*/\n\n/* Embed documents */\nconst documentRes = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\n/*\n[\n  [\n    -0.0047852774,  0.0048640342,   -0.01645707,  -0.024395779, -0.017263541,\n      0.012512918,  -0.019191515,   0.009053908,  -0.010213212, -0.026890801,\n      0.022883644,   0.010251015,  -0.023589306,  -0.006584088,  0.007989113,\n      0.002720268,   0.025088841,  -0.012153786,   0.012928754,  0.013054766,\n      -0.010395928, -0.0035566676,  0.0040008575,   0.008600268, -0.020678446,\n    -0.0019106456,   0.012178987,  -0.019241918,   0.030444318,  -0.03102397,\n      0.0035692686,  -0.007749692,   -0.00604854,   -0.01781799,  0.004860884,\n      -0.015612794,  0.0014097509,  -0.015637996,   0.019443536,  -0.01612944,\n      0.0072960514,   0.008316742,   0.011548932,  -0.013987249,  -0.03336778,\n      0.011341013,    0.00425603, -0.0126578305, -0.0013861238,  0.028302127,\n      0.025466874,  0.0007029065,  -0.016318457,   0.017427357, -0.016394064,\n      0.008499459,  -0.033241767,   0.031200387,    0.03238489,   -0.0212833,\n      0.0032416396,   0.005443686,  -0.007749692,  0.0060201874,  0.006281661,\n      0.016923312,   0.003528315,  0.0076740854,   -0.01881348,  0.026109532,\n      0.024660403,   0.005472039, -0.0016712243, -0.0048136297,  0.018397642,\n      0.003011669,  -0.011385117, -0.0020193304,   0.005138109, 0.0022335495,\n        0.03603922,  -0.027495656,  -0.008575066,   0.015436378,  0.018851284,","metadata":{"id":160}}],["161",{"pageContent":"0.003011669,  -0.011385117, -0.0020193304,   0.005138109, 0.0022335495,\n        0.03603922,  -0.027495656,  -0.008575066,   0.015436378,  0.018851284,\n      0.0018019609, -0.0034338066,    0.02094307,  -0.014503895, -0.024950229,\n      0.012632628,   0.013735226,  0.0069936244,   0.008575066, -0.015196957,\n    -0.0030541976,  -0.008745181,   0.016746895,  0.0040481114, -0.048010286,\n    ... 1436 more items\n  ],\n  [\n      -0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,\n      0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,\n      0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,\n      -0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,\n      -0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,\n      -0.022077737, -0.0009286407,   -0.02156674,   0.011890532,  -0.026283644,\n        0.02630985,   0.011942943,  -0.026126415,  -0.018264906,  -0.014045896,\n      -0.024187243,  -0.019037955,  -0.005037917,   0.020780588, -0.0049527506,\n      0.002399398,   0.020767486,  0.0080908025,  -0.019666875,  -0.027934562,\n      0.017688395,   0.015225122,  0.0046186363, -0.0045007137,   0.024265857,\n        0.03244183,  0.0038848957,   -0.03244183,  -0.018893827, -0.0018065092,\n      0.023440398,  -0.021763276,   0.015120302,   -0.01568371,  -0.010861984,\n      0.011739853,  -0.024501702,  -0.005214801,   0.022955606,   0.001315165,\n      -0.00492327,  0.0020358032,  -0.003468891,  -0.031079166,  0.0055259857,\n      0.0028547104,   0.012087069,   0.007992534, -0.0076256637,   0.008110457,\n      0.002998838,  -0.024265857,   0.006977089,  -0.015185814, -0.0069115767,\n      0.006466091,  -0.029428247,  -0.036241557,   0.036713246,   0.032284595,\n    -0.0021144184,  -0.014255536,   0.011228855,  -0.027227025,  -0.021619149,\n    0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,\n      0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,","metadata":{"id":161}}],["162",{"pageContent":"0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,\n      0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,\n    ... 1436 more items\n  ]\n]\n*/\n```\n\n## Dig deeper\n\n<DocCardList />","metadata":{"id":162}}],["163",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\n# Integrations: Embeddings\n\nLangChain offers a number of Embeddings implementations that integrate with various model providers. These are:\n\n## `OpenAIEmbeddings`\n\nThe `OpenAIEmbeddings` class uses the OpenAI API to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing `stripNewLines: false` to the constructor.\n\n```typescript\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\n\nconst embeddings = new OpenAIEmbeddings();\n```\n\n## `CohereEmbeddings`\n\n```bash npm2yarn\nnpm install cohere-ai\n```\n\n```typescript\nimport { CohereEmbeddings } from \"langchain/embeddings\";\n\nconst embeddings = new CohereEmbeddings();\n```","metadata":{"id":163}}],["164",{"pageContent":"---\nsidebar_position: 2\nhide_table_of_contents: true\nsidebar_label: Models\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Models\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/)\n:::\n\nModels are a core component of LangChain. LangChain is not a provider of models, but rather provides a standard interface through which you can interact with a variety of language models.\nLangChain provides support for both text-based Large Language Models (LLMs), Chat Models, and Text Embedding models.\n\nLLMs use a text-based input and output, while Chat Models use a message-based input and output.\n\n> **_Note:_** Chat model APIs are fairly new, so we are still figuring out the correct abstractions. If you have any feedback, please let us know!\n\n## All Models\n\n<DocCardList />\n\n## Advanced\n\n_This section is for users who want a deeper technical understanding of how LangChain works. If you are just getting started, you can skip this section._\n\nBoth LLMs and Chat Models are built on top of the `BaseLanguageModel` class. This class provides a common interface for all models, and allows us to easily swap out models in chains without changing the rest of the code.\n\nThe `BaseLanguageModel` class has two abstract methods: `generatePrompt` and `getNumTokens`, which are implemented by `BaseChatModel` and `BaseLLM` respectively.\n\n`BaseLLM` is a subclass of `BaseLanguageModel` that provides a common interface for LLMs while `BaseChatModel` is a subclass of `BaseLanguageModel` that provides a common interface for chat models.","metadata":{"id":164}}],["165",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/llm/llm.ts\";\nimport DebuggingExample from \"@examples/models/llm/llm_debugging.ts\";\nimport StreamingExample from \"@examples/models/llm/llm_streaming.ts\";\nimport TimeoutExample from \"@examples/models/llm/llm_timeout.ts\";\n\n# Additional Functionality: LLMs\n\nWe offer a number of additional features for LLMs. In most of the examples below, we'll be using the `OpenAI` LLM. However, all of these features are available for all LLMs.\n\n## Additional Methods\n\nLangChain provides a number of additional methods for interacting with LLMs:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Streaming Responses\n\nSome LLMs provide a streaming response. This means that instead of waiting for the entire response to be returned, you can start processing it as soon as it's available. This is useful if you want to display the response to the user as it's being generated, or if you want to process the response as it's being generated.\nLangChain currently provides streaming for the `OpenAI` LLM:\n\n<CodeBlock language=\"typescript\">{StreamingExample}</CodeBlock>\n\n## Caching\n\nLangChain provides an optional caching layer for LLMs. This is useful for two reasons:\n\n1. It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.\n2. It can speed up your application by reducing the number of API calls you make to the LLM provider.\n\n### Caching in-memory\n\nThe default cache is stored in-memory. This means that if you restart your application, the cache will be cleared.\n\nTo enable it you can pass `cache: true` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\n\nconst model = new OpenAI({ cache: true });\n```\n\n### Caching with Redis","metadata":{"id":165}}],["166",{"pageContent":"```typescript\nimport { OpenAI } from \"langchain/llms\";\n\nconst model = new OpenAI({ cache: true });\n```\n\n### Caching with Redis\n\nLangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the `redis` package:\n\n```bash npm2yarn\nnpm install redis\n```\n\nThen, you can pass a `cache` option when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\nimport { RedisCache } from \"langchain/cache\";\nimport { createClient } from \"redis\";\n\n// See https://github.com/redis/node-redis for connection options\nconst client = createClient();\nconst cache = new RedisCache(client);\n\nconst model = new OpenAI({ cache });\n```\n\n## Adding a timeout\n\nBy default, LangChain will wait indefinitely for a response from the model provider. If you want to add a timeout, you can pass a `timeout` option, in milliseconds, when you instantiate the model. For example, for OpenAI:\n\n<CodeBlock language=\"typescript\">{TimeoutExample}</CodeBlock>\n\nCurrently, the timeout option is only supported for OpenAI models.\n\n## Dealing with Rate Limits\n\nSome LLM providers have rate limits. If you exceed the rate limit, you'll get an error. To help you deal with this, LangChain provides a `maxConcurrency` option when instantiating an LLM. This option allows you to specify the maximum number of concurrent requests you want to make to the LLM provider. If you exceed this number, LangChain will automatically queue up your requests to be sent as previous requests complete.\n\nFor example, if you set `maxConcurrency: 5`, then LangChain will only send 5 requests to the LLM provider at a time. If you send 10 requests, the first 5 will be sent immediately, and the next 5 will be queued up. Once one of the first 5 requests completes, the next request in the queue will be sent.\n\nTo use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";","metadata":{"id":166}}],["167",{"pageContent":"To use this feature, simply pass `maxConcurrency: <number>` when you instantiate the LLM. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\n\nconst model = new OpenAI({ maxConcurrency: 5 });\n```\n\n## Dealing with API Errors\n\nIf the model provider returns an error from their API, by default LangChain will retry up to 6 times on an exponential backoff. This enables error recovery without any additional effort from you. If you want to change this behavior, you can pass a `maxRetries` option when you instantiate the model. For example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\n\nconst model = new OpenAI({ maxRetries: 10 });\n```\n\n## Logging for Debugging\n\nEspecially when using an agent, there can be a lot of back-and-forth going on behind the scenes as a LLM processes a chain. For agents, the response object contains an intermediateSteps object that you can print to see an overview of the steps it took to get there. If that's not enough and you want to see every exchange with the LLM, you can use the LLMCallbackManager to write yourself custom logging (or anything else you want to do) as the model goes through the steps:\n\n<CodeBlock language=\"typescript\">{DebuggingExample}</CodeBlock>","metadata":{"id":167}}],["168",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: LLMs\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/models/llm/llm_quick_start.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Getting Started: LLMs\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/models/language-model)\n:::\n\nLangChain provides a standard interface for using a variety of LLMs.\n\nTo get started, simply use the `call` method of an `LLM` implementation, passing in a `string` input. In this example, we are using the `OpenAI` implementation:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Dig deeper\n\n<DocCardList />","metadata":{"id":168}}],["169",{"pageContent":"---\nsidebar_position: 3\nsidebar_label: Integrations\n---\n\n# Integrations: LLMs\n\nLangChain offers a number of LLM implementations that integrate with various model providers. These are:\n\n## `OpenAI`\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\n\nconst model = new OpenAI({ temperature: 0.9 });\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```\n\n## `HuggingFaceInference`\n\n```bash npm2yarn\nnpm install @huggingface/inference\n```\n\n```typescript\nimport { HuggingFaceInference } from \"langchain/llms\";\n\nconst model = new HuggingFaceInference({ model: \"gpt2\" });\nconst res = await model.call(\"1 + 1 =\");\nconsole.log({ res });\n```\n\n## `Cohere`\n\n```bash npm2yarn\nnpm install cohere-ai\n```\n\n```typescript\nimport { Cohere } from \"langchain/llms\";\n\nconst model = new Cohere({ maxTokens: 20 });\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```\n\n## `Replicate`\n\n```bash npm2yarn\nnpm install replicate\n```\n\n```typescript\nimport { Replicate } from \"langchain/llms\";\n\nconst model = new Replicate({\n  model:\n    \"daanelson/flan-t5:04e422a9b85baed86a4f24981d7f9953e20c5fd82f6103b74ebc431588e1cec8\",\n});\nconst res = await modelA.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\nconsole.log({ res });\n```\n\n## Additional LLM Implementations\n\n### `PromptLayerOpenAI`\n\nLangChain integrates with PromptLayer for logging and debugging prompts and responses. To add support for PromptLayer:\n\n1. Create a PromptLayer account here: [https://promptlayer.com](https://promptlayer.com).\n2. Create an API token and pass it either as `promptLayerApiKey` argument in the `PromptLayerOpenAI` constructor or in the `PROMPTLAYER_API_KEY` environment variable.\n\n```typescript\nconst model = new PromptLayerOpenAI({ temperature: 0.9 });\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\n```","metadata":{"id":169}}],["170",{"pageContent":"```typescript\nconst model = new PromptLayerOpenAI({ temperature: 0.9 });\nconst res = await model.call(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\n```\n\nThe request and the response will be logged in the [PromptLayer dashboard](https://promptlayer.com/home).\n\n> **_Note:_** In streaming mode PromptLayer will not log the response.","metadata":{"id":170}}],["171",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Example Selectors\nsidebar_position: 3\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Example Selectors\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/example-selectors)\n:::\n\nIf you have a large number of examples, you may need to programmatically select which ones to include in the prompt. The ExampleSelector is the class responsible for doing so. The base interface is defined as below.\n\n```typescript\nclass BaseExampleSelector {\n  addExample(example: Example): Promise<void | string>;\n\n  selectExamples(input_variables: Example): Promise<Example[]>;\n}\n```\n\nIt needs to expose a `selectExamples` - this takes in the input variables and then returns a list of examples method - and an `addExample` method, which saves an example for later selection. It is up to each specific implementation as to how those examples are saved and selected. Letâ€™s take a look at some below.\n\n## Select by Length\n\nThis `ExampleSelector` selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.\n\nimport ExampleLength from \"@examples/prompts/length_based_example_selector.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleLength}</CodeBlock>\n\n## Select by Similarity\n\nThe `SemanticSimilarityExampleSelector` selects examples based on which examples are most similar to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.\n\nimport ExampleSimilarity from \"@examples/prompts/semantic_similarity_example_selector.ts\";\n\n<CodeBlock language=\"typescript\">{ExampleSimilarity}</CodeBlock>","metadata":{"id":171}}],["172",{"pageContent":"---\nsidebar_position: 3\nhide_table_of_contents: true\nsidebar_label: Prompts\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Prompts\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/prompts)\n:::\n\nLangChain provides several utilities to help manage prompts for language models, including chat models.\n\n<DocCardList />","metadata":{"id":172}}],["173",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Output Parsers\nsidebar_position: 2\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\n\n# Output Parsers\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/output-parser)\n:::\n\nLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.\n\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n\n- `getFormatInstructions(): str` A method which returns a string containing instructions for how the output of a language model should be formatted.\n- `parse(raw: string): any` A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\n\nAnd then one optional one:\n\n- `parseWithPrompt(text: string, prompt: BasePromptValue): any`: A method which takes in a string (assumed to be the response from a language model) and a formatted prompt (assumed to the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\n\nBelow we go over some examples of output parsers.\n\n## Structured Output Parser\n\nThis output parser can be used when you want to return multiple fields.\n\nimport Structured from \"@examples/prompts/structured_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Structured}</CodeBlock>\n\n## Structured Output Parser with Zod Schema\n\nThis output parser can be also be used when you want to define the output schema using Zod, a TypeScript validation library. The Zod schema passed in needs be parseable from a JSON string, so eg. `z.date()` is not allowed, but `z.coerce.date()` is.\n\nimport StructuredZod from \"@examples/prompts/structured_parser_zod.ts\";\n\n<CodeBlock language=\"typescript\">{StructuredZod}</CodeBlock>\n\n## Output Fixing Parser","metadata":{"id":173}}],["174",{"pageContent":"import StructuredZod from \"@examples/prompts/structured_parser_zod.ts\";\n\n<CodeBlock language=\"typescript\">{StructuredZod}</CodeBlock>\n\n## Output Fixing Parser\n\nThis output parser wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.\n\nimport Fix from \"@examples/prompts/fix_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Fix}</CodeBlock>\n\n## Comma-separated List Parser\n\nThis output parser can be used when you want to return a list of items.\n\nimport Comma from \"@examples/prompts/comma_list_parser.ts\";\n\n<CodeBlock language=\"typescript\">{Comma}</CodeBlock>","metadata":{"id":174}}],["175",{"pageContent":"---\nsidebar_label: Additional Functionality\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport PromptValue from \"@examples/prompts/prompt_value.ts\";\nimport PartialValue from \"@examples/prompts/partial.ts\";\nimport FewShot from \"@examples/prompts/few_shot.ts\";\n\n# Additional Functionality: Prompt Templates\n\nWe offer a number of extra features for prompt templates, as shown below:\n\n## Prompt Values\n\nA `PromptValue` is an object returned by the `formatPromptValue` of a `PromptTemplate`. It can be converted to a string or list of `ChatMessage` objects.\n\n<CodeBlock language=\"typescript\">{PromptValue}</CodeBlock>\n\n## Partial Values\n\nLike other methods, it can make sense to \"partial\" a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\n\nLangChain supports this in two ways:\n\n1. Partial formatting with string values.\n2. Partial formatting with functions that return string values.\n\nThese two different ways support different use cases. In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.\n\n<CodeBlock language=\"typescript\">{PartialValue}</CodeBlock>\n\n## Few-Shot Prompt Templates\n\nA few-shot prompt template is a prompt template you can build with examples.\n\n<CodeBlock language=\"typescript\">{FewShot}</CodeBlock>","metadata":{"id":175}}],["176",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_label: Prompt Templates\nsidebar_position: 1\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/prompts/prompts.ts\";\nimport DocCardList from \"@theme/DocCardList\";\n\n# Prompt Templates\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/components/prompts/prompt-template)\n:::\n\nA `PromptTemplate` allows you to make use of templating to generate a prompt. This is useful for when you want to use the same prompt outline in multiple places, but with certain values changed.\nPrompt templates are supported for both LLMs and chat models, as shown below:\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n\n## Dig deeper\n\n<DocCardList />","metadata":{"id":176}}],["177",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\n# Chat Messages\n\nThe primary interface through which end users interact with LLMs is a chat interface. For this reason, some model providers have started providing access to the underlying API in a way that expects chat messages. These messages have a content field (which is usually text) and are associated with a user (or role). Right now the supported users are System, Human, and AI.\n\n## SystemChatMessage\n\nA chat message representing information that should be instructions to the AI system.\n\n```typescript\nnew SystemChatMessage(\"You are a nice assistant\");\n```\n\n## HumanChatMessage\n\nA chat message representing information coming from a human interacting with the AI system.\n\n```typescript\nnew HumanChatMessage(\"Hello, how are you?\");\n```\n\n## AIChatMessage\n\nA chat message representing information coming from the AI system.\n\n```typescript\nnew AIChatMessage(\"I am doing well, thank you!\");\n```","metadata":{"id":177}}],["178",{"pageContent":"# Document\n\nLanguage models only know information about what they were trained on. In order to get them answer questions or summarize other information you have to pass it to the language model. Therefore, it is very important to have a concept of a document.\n\nA document at its core is fairly simple. It consists of a piece of text and optional metadata. The piece of text is what we interact with the language model, while the optional metadata is useful for keeping track of metadata about the document (such as the source).\n\n```typescript\ninterface Document {\n  pageContent: string;\n  metadata: Record<string, any>;\n}\n```\n\n## Creating a Document\n\nYou can create a document object rather easily in LangChain with:\n\n```typescript\nimport { Document } from \"langchain/document\";\n\nconst doc = new Document({ pageContent: \"foo\" });\n```\n\nYou can create one with metadata with:\n\n```typescript\nimport { Document } from \"langchain/document\";\n\nconst doc = new Document({ pageContent: \"foo\", metadata: { source: \"1\" } });\n```\n\nAlso check out [Document Loaders](../indexes/document_loaders/) for a way to load documents from a variety of sources.","metadata":{"id":178}}],["179",{"pageContent":"---\n---\n\n# Examples\n\nExamples are input/output pairs that represent inputs to a function and then expected output. They can be used in both training and evaluation of models.\n\n```typescript\ntype Example = Record<string, string>;\n```\n\n## Creating an Example\n\nYou can create an Example like this:\n\n```typescript\nconst example = {\n  input: \"foo\",\n  output: \"bar\",\n};\n```","metadata":{"id":179}}],["180",{"pageContent":"---\nsidebar_position: 1\n---\n\nimport DocCardList from \"@theme/DocCardList\";\n\n# Schema\n\nThis section speaks about interfaces that are used throughout the rest of the library.\n\n<DocCardList />","metadata":{"id":180}}],["181",{"pageContent":"import CodeBlock from \"@theme/CodeBlock\";\n\n# Events / Callbacks\n\nLangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, [monitoring](./tracing), [streaming](../modules/models/llms/additional_functionality#streaming-responses), and other tasks.\n\nYou can subscribe to these events by using the `callbackManager` argument available throughout the API. A `CallbackManager` is an object that manages a list of `CallbackHandlers`. The `CallbackManager` will call the appropriate method on each handler when the event is triggered.\n\n```typescript\ninterface CallbackManager {\n  addHandler(handler: CallbackHandler): void;\n\n  removeHandler(handler: CallbackHandler): void;\n\n  setHandlers(handlers: CallbackHandler[]): void;\n\n  setHandler(handler: CallbackHandler): void;\n}\n```\n\nCallbackHandlers are objects that implement the `CallbackHandler` interface, which has a method for each event that can be subscribed to. The `CallbackManager` will call the appropriate method on each handler when the event is triggered.\n\n```typescript\nabstract class BaseCallbackHandler {\n  handleLLMStart?(\n    llm: { name: string },\n    prompts: string[],\n    verbose?: boolean\n  ): Promise<void>;\n\n  handleLLMNewToken?(token: string, verbose?: boolean): Promise<void>;\n\n  handleLLMError?(err: Error, verbose?: boolean): Promise<void>;\n\n  handleLLMEnd?(output: LLMResult, verbose?: boolean): Promise<void>;\n\n  handleChainStart?(\n    chain: { name: string },\n    inputs: ChainValues,\n    verbose?: boolean\n  ): Promise<void>;\n\n  handleChainError?(err: Error, verbose?: boolean): Promise<void>;\n\n  handleChainEnd?(outputs: ChainValues, verbose?: boolean): Promise<void>;\n\n  handleToolStart?(\n    tool: { name: string },\n    input: string,\n    verbose?: boolean\n  ): Promise<void>;\n\n  handleToolError?(err: Error, verbose?: boolean): Promise<void>;\n\n  handleToolEnd?(output: string, verbose?: boolean): Promise<void>;\n\n  handleText?(text: string, verbose?: boolean): Promise<void>;","metadata":{"id":181}}],["182",{"pageContent":"handleToolError?(err: Error, verbose?: boolean): Promise<void>;\n\n  handleToolEnd?(output: string, verbose?: boolean): Promise<void>;\n\n  handleText?(text: string, verbose?: boolean): Promise<void>;\n\n  handleAgentAction?(action: AgentAction, verbose?: boolean): Promise<void>;\n\n  handleAgentEnd?(action: AgentFinish, verbose?: boolean): Promise<void>;\n}\n```\n\n## Using an existing handler\n\nLangChain provides a few built-in handlers that you can use to get started. These are available in the `langchain/callbacks` module. The most basic handler is the `ConsoleCallbackHandler`, which simply logs all events to the console. In the future we will add more default handlers to the library.\n\nimport ConsoleExample from \"@examples/callbacks/console_handler.ts\";\n\n<CodeBlock language=\"typescript\">{ConsoleExample}</CodeBlock>\n\n## Creating a one-off handler\n\nWe offer a method on the `CallbackManager` class that allows you to create a one-off handler. This is useful if eg. you need to create a handler that you will use only for a single request, eg to stream the output of an LLM/Agent/etc to a websocket.\n\nThis is a more complete example that passes a `CallbackManager` to a ChatModel, and LLMChain, a Tool, and an Agent.\n\nimport AgentExample from \"@examples/agents/streaming.ts\";\n\n<CodeBlock language=\"typescript\">{AgentExample}</CodeBlock>\n\n## Creating a custom handler\n\nYou can also create your own handler by implementing the `CallbackHandler` interface. This is useful if you want to do something more complex than just logging to the console, eg. send the events to a logging service. As an example here is the implementation of the `ConsoleCallbackHandler`:\n\n```typescript\nexport class MyCallbackHandler extends BaseCallbackHandler {\n  async handleChainStart(chain: { name: string }) {\n    console.log(`Entering new ${chain.name} chain...`);\n  }\n\n  async handleChainEnd(_output: ChainValues) {\n    console.log(\"Finished chain.\");\n  }\n\n  async handleAgentAction(action: AgentAction) {\n    console.log(action.log);\n  }","metadata":{"id":182}}],["183",{"pageContent":"}\n\n  async handleChainEnd(_output: ChainValues) {\n    console.log(\"Finished chain.\");\n  }\n\n  async handleAgentAction(action: AgentAction) {\n    console.log(action.log);\n  }\n\n  async handleToolEnd(output: string) {\n    console.log(output);\n  }\n\n  async handleText(text: string) {\n    console.log(text);\n  }\n\n  async handleAgentEnd(action: AgentFinish) {\n    console.log(action.log);\n  }\n}\n```\n\nYou could then use it as described in the [section](#using-an-existing-handler) above.","metadata":{"id":183}}],["184",{"pageContent":"# Deployment\n\nYou've built your LangChain app and now you're looking to deploy it to production? You've come to the right place. This guide will walk you through the options you have for deploying your app, and the considerations you should make when doing so.\n\n## Overview\n\nLangChain is a library for building applications that use language models. It is not a web framework, and does not provide any built-in functionality for serving your app over the web. Instead, it provides a set of tools that you can integrate in your API or backend server.\n\nThere are a couple of high-level options for deploying your app:\n\n- Deploying to a VM or container\n  - Persistent filesystem means you can save and load files from disk\n  - Always-running process means you can cache some things in memory\n  - You can support long-running requests, such as WebSockets\n- Deploying to a serverless environment\n  - No persistent filesystem means you can load files from disk, but not save them for later\n  - Cold start means you can't cache things in memory and expect them to be cached between requests\n  - Function timeouts mean you can't support long-running requests, such as WebSockets\n\nSome other considerations include:\n\n- Do you deploy your backend and frontend together, or separately?\n- Do you deploy your backend co-located with your database, or separately?\n\nAs you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out [this form](https://forms.gle/57d8AmXBYp8PP8tZA) and we'll set up a dedicated support Slack channel.\n\n## Deployment Options\n\nSee below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.\n\n### Deploying to Fly.io\n\n[Fly.io](https://fly.io) is a platform for deploying apps to the cloud. It's a great option for deploying your app to a container environment.","metadata":{"id":184}}],["185",{"pageContent":"### Deploying to Fly.io\n\n[Fly.io](https://fly.io) is a platform for deploying apps to the cloud. It's a great option for deploying your app to a container environment.\n\nSee [our Fly.io template](https://github.com/hwchase17/langchain-template-node-fly) for an example of how to deploy your app to Fly.io.","metadata":{"id":185}}],["186",{"pageContent":"# Tracing\n\nSimilar to the Python `langchain` package, JS `langchain` also supports tracing.\n\nYou can view an overview of tracing [here.](https://langchain.readthedocs.io/en/latest/tracing.html)\nTo spin up the tracing backend, run `docker compose up` (or `docker-compose up` if on using an older version of `docker`) in the `langchain` directory.\nYou can also use the `langchain-server` command if you have the python `langchain` package installed.\n\nHere's an example of how to use tracing in `langchain.js`. All that needs to be done is setting the `LANGCHAIN_HANDLER` environment variable to `langchain`.\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport { initializeAgentExecutor } from \"langchain/agents\";\nimport { SerpAPI, Calculator } from \"langchain/tools\";\nimport process from \"process\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [new SerpAPI(), new Calculator()];\n\n  const executor = await initializeAgentExecutor(\n    tools,\n    model,\n    \"zero-shot-react-description\",\n    true\n  );\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```\n\nWe are actively working on improving tracing to work better with concurrency. For now, the best way to use tracing with concurrency is to follow the below example:\n\n```typescript\nimport { OpenAI } from \"langchain\";\nimport { initializeAgentExecutor } from \"langchain/agents\";\nimport { SerpAPI, Calculator } from \"langchain/tools\";\nimport process from \"process\";\nimport {\n  CallbackManager,\n  LangChainTracer,\n  ConsoleCallbackHandler,\n} from \"langchain/callbacks\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [new SerpAPI(), new Calculator()];","metadata":{"id":186}}],["187",{"pageContent":"export const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [new SerpAPI(), new Calculator()];\n\n  const executor = await initializeAgentExecutor(\n    tools,\n    model,\n    \"zero-shot-react-description\",\n    true\n  );\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  // This will result in a lot of errors, because the shared Tracer is not concurrency-safe.\n  const [resultA, resultB, resultC] = await Promise.all([\n    executor.call({ input }),\n    executor.call({ input }),\n    executor.call({ input }),\n  ]);\n\n  console.log(`Got output ${resultA.output}`);\n  console.log(`Got output ${resultB.output}`);\n  console.log(`Got output ${resultC.output}`);\n\n  // This will work, because each executor has its own Tracer, avoiding concurrency issues.\n  console.log(\"---Now with concurrency-safe tracing---\");\n\n  const executors = [];\n  for (let i = 0; i < 3; i += 1) {\n    const callbackManager = new CallbackManager();\n    callbackManager.addHandler(new ConsoleCallbackHandler());\n    callbackManager.addHandler(new LangChainTracer());\n\n    const model = new OpenAI({ temperature: 0, callbackManager });\n    const tools = [new SerpAPI(), new Calculator()];\n    for (const tool of tools) {\n      tool.callbackManager = callbackManager;\n    }\n    const executor = await initializeAgentExecutor(\n      tools,\n      model,\n      \"zero-shot-react-description\",\n      true,\n      callbackManager\n    );\n    executor.agent.llmChain.callbackManager = callbackManager;\n    executors.push(executor);\n  }\n\n  const results = await Promise.all(\n    executors.map((executor) => executor.call({ input }))\n  );\n  for (const result of results) {\n    console.log(`Got output ${result.output}`);\n  }\n};\n```","metadata":{"id":187}}],["188",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 4\n---\n\n# Interacting with APIs\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/apis)\n:::\n\nLots of data and information is stored behind APIs.\nThis page covers all resources available in LangChain for working with APIs.\n\n## Chains\n\nIf you are just getting started, and you have relatively apis, you should get started with chains.\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\nunderstand what is happening better.\n\nTODO: add an API chain and then add an example here.\n\n## Agents\n\nAgents are more complex, and involve multiple queries to the LLM to understand what to do.\nThe downside of agents are that you have less control. The upside is that they are more powerful,\nwhich allows you to use them on larger and more complex schemas.\n\n- [OpenAPI Agent](../modules/agents/toolkits/examples/openapi.md)","metadata":{"id":188}}],["189",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\n# Personal Assistants\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/personal-assistants)\n:::\n\nWe use \"personal assistant\" here in a very broad sense.\nPersonal assistants have a few characteristics:\n\n- They can interact with the outside world\n- They have knowledge of your data\n- They remember your interactions\n\nReally all of the functionality in LangChain is relevant for building a personal assistant.\nHighlighting specific parts:\n\n- [Agent Documentation](../modules/agents/index.mdx) (for interacting with the outside world)\n- [Index Documentation](../modules/indexes/index.mdx) (for giving them knowledge of your data)\n- [Memory](../modules/memory/index.mdx) (for helping them remember interactions)","metadata":{"id":189}}],["190",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 2\n---\n\n# Question Answering\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/qa-docs)\n:::\n\nQuestion answering in this context refers to question answering over your document data.\nThere are a few different types of question answering:\n\n- [Retrieval Question Answering](../modules/chains/index_related_chains/retrieval_qa): Use this to ingest documents, index them into a vectorstore, and then be able to ask questions about it.\n- [Chat Retrieval](../modules/chains/index_related_chains/conversational_retrieval): Similar to above in that you ingest and index documents, but this lets you have more a conversation (ask follow up questions, etc) rather than just asking one-off questions.\n\n## Indexing\n\nFor question answering over many documents, you almost always want to create an index over the data.\nThis can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money).\n\nTherefor, it is really important to understand how to create indexes, and so you should familiarize yourself with all the documentation related to that.\n\n- [Indexes](../modules/indexes/index.mdx)\n\n## Chains\n\nAfter you create an index, you can then use it in a chain.\nYou can just do normal question answering over it, or you can use it a conversational way.\nFor an overview of these chains (and more) see the below documentation.\n\n- [Index related chains](../modules/chains/index_related_chains/index.mdx)\n\n## Agents\n\nIf you want to be able to answer more complex, multi-hop questions you should look into combining your indexes with an agent.\nFor an example of how to do that, please see the below.\n\n- [Vectorstore Agent](../modules/agents/toolkits/examples/vectorstore.md)","metadata":{"id":190}}],["191",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 6\n---\n\n# Summarization\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/summarization)\n:::\n\nA common use case is wanting to summarize long documents.\nThis naturally runs into the context window limitations.\nUnlike in question-answering, you can't just do some semantic search hacks to only select the chunks of text most relevant to the question (because, in this case, there is no particular question - you want to summarize everything).\nSo what do you do then?\n\nTo get started, we would recommend checking out the summarization chain which attacks this problem in a recursive manner.\n\n- [Summarization Chain](../modules/chains/other_chains/summarization)","metadata":{"id":191}}],["192",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 3\n---\n\n# Tabular Question Answering\n\n:::info\n[Conceptual Guide](https://docs.langchain.com/docs/use-cases/qa-tabular)\n:::\n\nLots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.\nThis page covers all resources available in LangChain for working with data in this format.\n\n## Chains\n\nIf you are just getting started, and you have relatively small/simple tabular data, you should get started with chains.\nChains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you\nunderstand what is happening better.\n\n- [SQL Database Chain](../modules/chains/other_chains/sql)\n\n## Agents\n\nAgents are more complex, and involve multiple queries to the LLM to understand what to do.\nThe downside of agents are that you have less control. The upside is that they are more powerful,\nwhich allows you to use them on larger databases and more complex schemas.\n\n- [SQL Agent](../modules/agents/toolkits/examples/sql.mdx)","metadata":{"id":192}}],["193",{"pageContent":"# Website\n\nThis website is built using [Docusaurus 2](https://docusaurus.io/), a modern static website generator.\n\n### Installation\n\n```\n$ yarn\n```\n\n### Local Development\n\n```\n$ yarn start\n```\n\nThis command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.\n\n### Build\n\n```\n$ yarn build\n```\n\nThis command generates static content into the `build` directory and can be served using any static contents hosting service.\n\n### Deployment\n\nUsing SSH:\n\n```\n$ USE_SSH=true yarn deploy\n```\n\nNot using SSH:\n\n```\n$ GIT_USER=<Your GitHub username> yarn deploy\n```\n\nIf you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.\n\n### Continuous Integration\n\nSome common defaults for linting/formatting have been set for you. If you integrate your project with an open source Continuous Integration system (e.g. Travis CI, CircleCI), you may check for issues using the following command.\n\n```\n$ yarn ci\n```","metadata":{"id":193}}]]